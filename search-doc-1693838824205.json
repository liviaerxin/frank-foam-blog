[{"title":"Awesome Troubleshooting","type":0,"sectionRef":"#","url":"/blog/awesome-troubleshooting","content":"A curated collection of troubleshooting practices: How we spent two weeks hunting an NFS bug in the Linux kernel","keywords":"best-practice troubleshooting"},{"title":"Avalonia","type":0,"sectionRef":"#","url":"/blog/avalonia","content":"","keywords":"practice avalonia"},{"title":"SkiaSharp​","type":1,"pageTitle":"Avalonia","url":"/blog/avalonia#skiasharp","content":"SkiaSharp/GRContextTest.cs at main · mono/SkiaSharp · GitHub "},{"title":"C++ Package Management","type":0,"sectionRef":"#","url":"/blog/c-package-management","content":"vcpkg [learn-vcpkg] conan","keywords":"c++ package vcpkg conan"},{"title":"Code Snippet Management","type":0,"sectionRef":"#","url":"/blog/code-snippet-management","content":"","keywords":"code snippet manager"},{"title":"C/C++ Build System","type":0,"sectionRef":"#","url":"/blog/c-build-system","content":"","keywords":"practice avalonia"},{"title":"Build System​","type":1,"pageTitle":"C/C++ Build System","url":"/blog/c-build-system#build-system","content":"Make Ninja MSBuild "},{"title":"Build System Generator​","type":1,"pageTitle":"C/C++ Build System","url":"/blog/c-build-system#build-system-generator","content":"CMake Meson "},{"title":"CI​","type":1,"pageTitle":"C/C++ Build System","url":"/blog/c-build-system#ci","content":""},{"title":"CIL(Common Intermediate Language)","type":0,"sectionRef":"#","url":"/blog/common-intermediate-language","content":"","keywords":""},{"title":"What is CIl/IL​","type":1,"pageTitle":"CIL(Common Intermediate Language)","url":"/blog/common-intermediate-language#what-is-cilil","content":"For C# or Java, the program is not directly compiled to machine code, but intermediate language code. For C#, the intermediate code is called Common Intermediate Language(CIL, or IL). So whether the *.dll or *.exe compiled from C#, is composed of IL code and its corresponding meta data. At runtime, the JIT(Just-In-Compiler) compile the IL code to the native machine code. "},{"title":"What is JIT​","type":1,"pageTitle":"CIL(Common Intermediate Language)","url":"/blog/common-intermediate-language#what-is-jit","content":"./ilasm ~/Documents/peggy-foam-wiki/docs/IL/test/test.il -dll ./ildasm ~/Documents/peggy-foam-wiki/docs/IL/test/test.dll -t dotnet myapp.dll  "},{"title":"Tools","type":0,"sectionRef":"#","url":"/blog/diagram-tools","content":"","keywords":""},{"title":"Generic Tools​","type":1,"pageTitle":"Tools","url":"/blog/diagram-tools#generic-tools","content":"drawio: draw.io is a JavaScript, client-side editor for general diagramming and whiteboarding Intelligent Diagramming | Lucidchart The Visual Collaboration Platform for Every Team | Miro "},{"title":"Code as Diagram Tools​","type":1,"pageTitle":"Tools","url":"/blog/diagram-tools#code-as-diagram-tools","content":"GitHub - mermaid-js/mermaid: Generation of diagrams like flowcharts or sequence diagrams from text in a similar manner as markdown GitHub - plantuml/plantuml: Generate diagrams from textual description "},{"title":"ERD Diagram Tools​","type":1,"pageTitle":"Tools","url":"/blog/diagram-tools#erd-diagram-tools","content":""},{"title":"Data Center","type":0,"sectionRef":"#","url":"/blog/data-center","content":"","keywords":""},{"title":"Economic Data​","type":1,"pageTitle":"Data Center","url":"/blog/data-center#economic-data","content":"FRED ECONOMIC DATA US inflation cpi "},{"title":"Hello from Docusaurus","type":0,"sectionRef":"#","url":"/blog/doc-with-tags","content":"","keywords":""},{"title":"Headers​","type":1,"pageTitle":"Hello from Docusaurus","url":"/blog/doc-with-tags#headers","content":"will show up on the table of contents on the upper right So that your users will know what this page is all about without scrolling down or even without reading too much. "},{"title":"Only h2 and h3 will be in the TOC by default.​","type":1,"pageTitle":"Hello from Docusaurus","url":"/blog/doc-with-tags#only-h2-and-h3-will-be-in-the-toc-by-default","content":"You can configure the TOC heading levels either per-document or in the theme configuration. The headers are well-spaced so that the hierarchy is clear. lists will help youpresent the key pointsthat you want your users to remember and you may nest them multiple times "},{"title":"Share Data between Docker Containers","type":0,"sectionRef":"#","url":"/blog/docker-containers-data-sharing","content":"","keywords":"Docker Containters Data Sharing"},{"title":"Use a volume to bind a local folder​","type":1,"pageTitle":"Share Data between Docker Containers","url":"/blog/docker-containers-data-sharing#use-a-volume-to-bind-a-local-folder","content":"In default, the volume is created by Docker and its corresponding folder resides in Docker managed folder like /var/lib/docker/volumes/: $ docker create volume xxx  $ docker volume inspect xxx [ { &quot;CreatedAt&quot;: &quot;2023-07-19T14:41:18+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/xxx/_data&quot;, &quot;Name&quot;: &quot;xxx&quot;, &quot;Options&quot;: {}, &quot;Scope&quot;: &quot;local&quot; } ]  However, sometimes you would like to bind the volume into a specified local folder(like /data/volumes/testvol) in hosts(only available in Linux) $ docker volume create --opt type=none --opt o=bind --opt device=/data/volumes/testvol testvol  $ docker inspect testvol [ { &quot;CreatedAt&quot;: &quot;2023-07-13T04:36:16Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/testvol/_data&quot;, &quot;Name&quot;: &quot;testvol&quot;, &quot;Options&quot;: { &quot;device&quot;: &quot;/data/volumes/testvol&quot;, &quot;o&quot;: &quot;bind&quot;, &quot;type&quot;: &quot;none&quot; }, &quot;Scope&quot;: &quot;local&quot; }  In Docker compose yaml, services: frontend: image: node:lts volumes: - testvol:/home/node/app volumes: db-data: testvol: driver: local driver_opts: type: none o: bind device: /data/volumes/testvol  "},{"title":"Use NFS volume​","type":1,"pageTitle":"Share Data between Docker Containers","url":"/blog/docker-containers-data-sharing#use-nfs-volume","content":""},{"title":"Use Samba volume​","type":1,"pageTitle":"Share Data between Docker Containers","url":"/blog/docker-containers-data-sharing#use-samba-volume","content":""},{"title":"References​","type":1,"pageTitle":"Share Data between Docker Containers","url":"/blog/docker-containers-data-sharing#references","content":"Volumes | Docker Documentation "},{"title":"Communication Between Docker Containers","type":0,"sectionRef":"#","url":"/blog/docker-containers-communication","content":"","keywords":""},{"title":"Using --link flag(Legacy)​","type":1,"pageTitle":"Communication Between Docker Containers","url":"/blog/docker-containers-communication#using---link-flaglegacy","content":"Start a postgres db container: docker run --rm --name postgres-db --detach -e POSTGRES_PASSWORD=mysecretpassword postgres  Run a postgres client container to connect the db container with user postgres and password mysecretpassword: docker run -it --rm --link postgres-db:db postgres psql -h db -U postgres psql (14.3) Type &quot;help&quot; for help. postgres=# SELECT 1; ?column? ---------- 1 (1 row)  Or run a utility container: docker run -it --rm --link postgres-db:db busybox sh # in `busybox` ping db  "},{"title":"Using Default Bridge Network​","type":1,"pageTitle":"Communication Between Docker Containers","url":"/blog/docker-containers-communication#using-default-bridge-network","content":"If you are running your container without specifying attached network, it will use the docker default bridge network. However The default bridge network allows container-to-container communication by IP address only. To use hostname or alias name in connecting rather than IP address, see the following methods. So before connecting, we need get the container IP address by docker inspect. Start a postgres db container: docker run --rm --name postgres-db --detach -e POSTGRES_PASSWORD=mysecretpassword postgres  Get the IP address of the postgres db container: docker inspect mynginx | grep IPAddress &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,  Run a postgres client container to connect the db container: docker run -it --rm postgres psql -h &quot;172.17.0.2&quot; -U postgres psql (14.3) Type &quot;help&quot; for help. postgres=# SELECT 1; ?column? ---------- 1 (1 row)  "},{"title":"Using Private Defined Bridge Network​","type":1,"pageTitle":"Communication Between Docker Containers","url":"/blog/docker-containers-communication#using-private-defined-bridge-network","content":"The private defined bridge network will give you more privacy that it only allows only containers belonging to it can talk to each other. Moreover, you can use hostname or alias name to connect without regard of IP address changing due to re-start. Create a private bridge network: docker network create postgres-net  Start a postgres db container: docker run --rm --net postgres-net --name postgres-db --detach -e POSTGRES_PASSWORD=mysecretpassword postgres  Run a postgres client container to connect the db container: docker run -it --rm --net postgres-net postgres psql -h postgres-db -U postgres psql (14.3) Type &quot;help&quot; for help. postgres=# SELECT 1; ?column? ---------- 1 (1 row)  "},{"title":"Use Case in Docker Compose​","type":1,"pageTitle":"Communication Between Docker Containers","url":"/blog/docker-containers-communication#use-case-in-docker-compose","content":"Actually, docker compose will create its private bridge network, and when it start containers, containers will be attached to that network in default. docker-compose-postgres.yml # Use below credentials to access in `adminer` web to access `db`, # server: db (db1, db2 are also available!) # user: postgres # password: example version: '3.1' services: db: image: postgres restart: always environment: # POSTGRES_USER: postgres # `postgres` in default. POSTGRES_PASSWORD: example networks: default: aliases: - db1 - db2 adminer: image: adminer restart: always ports: - 8080:8080  "},{"title":"Set Up Samba Server in Docker","type":0,"sectionRef":"#","url":"/blog/docker-setup-samba-server","content":"","keywords":"Set Up Samba Server in Docker"},{"title":"Start Samba Server in Docker​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/blog/docker-setup-samba-server#start-samba-server-in-docker","content":"Here, we use Samba server image from dperson/samba. Although there is an alternative from ghcr.io/servercontainers/samba or crazymax/samba In OSX, it's critical to use volume mount and avoid using bind mount as we mentioned above. note In OSX, due to the docker desktop itself is running in VM, it will cause some error like Operation not supported when binding a local file folder via bind mount even you set 777 mask on the folder. So it's recommended to use volume mount to bind to /mnt in Samba server in OSX. Furthermore, the Samba server will log such message: error reading meta xattr: Not supported. In Linux, it's okay to use either volume mount or bind mount. docker run -it --rm \\ --name samba \\ -p 139:139 -p 445:445 \\ -v mnt:/mnt:z \\ dperson/samba \\ -p -s &quot;Mount;/mnt;yes;no;yes&quot; -u &quot;bob;bobspasswd&quot; -g &quot;log level = 5&quot;  note -s &quot;&lt;Mount;/mnt&gt;;yes;no;yes&quot; means [browsable:yes;readonly:no;guest:yes]&quot;, which will allow the guest to read and the user to read/write! Get the samba server IP address: $ docker inspect \\ -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' \\ samba 172.17.0.2  "},{"title":"Start Samba Client in Docker​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/blog/docker-setup-samba-server#start-samba-client-in-docker","content":"Use dperson/samba or busybox image, docker run -it --rm --privileged dperson/samba bash  or docker run -it --rm --privileged busybox sh  Inside the container: mkdir /smb_share # mount -t cifs //[server-ip]/[share-path] /[mount-point] mount -t cifs //172.17.0.2/Mount /smb_share -o rw,username=bob,password=bobspasswd # write file echo &quot;xxxx&quot; &gt; /smb_share/f.txt  "},{"title":"Start Samba Client which Create Volume in Docker​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/blog/docker-setup-samba-server#start-samba-client-which-create-volume-in-docker","content":"Create a CIFS/Samba Volume docker volume create \\ --driver local \\ --opt type=cifs \\ --opt device=//172.17.0.2/Mount \\ --opt o=username=bob,password=bobspasswd \\ --name samba-volume  $ docker inspect samba-volume [ { &quot;CreatedAt&quot;: &quot;2023-08-13T16:24:03Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/samba-volume/_data&quot;, &quot;Name&quot;: &quot;samba-volume&quot;, &quot;Options&quot;: { &quot;device&quot;: &quot;//172.17.0.2/Mount&quot;, &quot;o&quot;: &quot;addr=username=bob,password=bobspasswd&quot;, &quot;type&quot;: &quot;cifs&quot; }, &quot;Scope&quot;: &quot;local&quot; } ]  Start a container using the created volume samba-volume. docker run -it --rm \\ -v samba-volume:/mnt \\ busybox \\ sh  "},{"title":"Use Case in Docker Compose​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/blog/docker-setup-samba-server#use-case-in-docker-compose","content":" docker-compose-samba.yml version: &quot;3&quot; # Inspired by https://github.com/dperson/samba/blob/master/docker-compose.yml services: samba: image: dperson/samba privileged: true environment: TZ: &quot;EST5EDT&quot; ports: - &quot;137:137/udp&quot; - &quot;138:138/udp&quot; - &quot;139:139/tcp&quot; - &quot;445:445/tcp&quot; volumes: - mnt:/mnt:z command: '-s &quot;Mount;/mnt;yes;no;yes&quot; -u &quot;bob;bobspasswd&quot; -p' # networks: # default: # ipv4_address: 172.28.0.20 samba-client: image: busybox restart: on-failure volumes: - samba-volume:/smb_share # tty: true command: sleep infinity depends_on: - samba # networks: # default: # ipv4_address: 172.28.0.21 # networks: # default: # driver: bridge # ipam: # config: # - subnet: 172.28.0.0/16 # gateway: 172.28.0.1 volumes: mnt: samba-volume: driver: local driver_opts: type: cifs # device: &quot;//172.28.0.20/Mount&quot; # o: &quot;username=bob,password=bobspasswd&quot; device: &quot;//localhost/Mount&quot; o: &quot;addr=localhost,username=bob,password=bobspasswd&quot;  "},{"title":"Troubleshooting​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/blog/docker-setup-samba-server#troubleshooting","content":"When you mount an Samba Share in Linux, you may encounter error like failed: Invalid argument, bash-5.1# mount -t cifs //172.17.0.2/Mount /mnt/smb_share -o iocharset=utf8,rw,vers=1.0 mount: mounting //172.17.0.2/Mount on /mnt/smb_share failed: Invalid argument  You can use dmesg to debug, bash-5.1# dmesg [317258.750535] CIFS: Attempting to mount \\\\172.17.0.2\\Mount [317258.752956] CIFS: VFS: No username specified [317336.240984] cifs: Unknown parameter 'passwd' [317344.451345] CIFS: Attempting to mount \\\\172.17.0.2\\Mount  "},{"title":"Set Up NFS Sever in Docker","type":0,"sectionRef":"#","url":"/blog/docker-setup-nfs-sever","content":"","keywords":"Setup NFS Sever"},{"title":"Start Up NFS Server​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/blog/docker-setup-nfs-sever#start-up-nfs-server","content":"Use docker image gists/nfs-server to start up a NFS server container. In OSX, it's critical to use volume mount and avoid using bind mount as we mentioned above. In Linux, it's okay to use either volume mount or bind mount. docker run --rm -d \\ --name nfs \\ --privileged \\ -p 2049:2049 \\ -v /tmp/volume:/nfs-share \\ -e NFS_DIR=/nfs-share \\ -e NFS_OPTION=&quot;rw,fsid=0,async,no_subtree_check,no_auth_nlm,insecure,no_root_squash&quot; \\ gists/nfs-server  note Before we use an old nfs server image itsthenetwork/nfs-server-alpine which was not maintained more than 4 years and not supported natively in platform linux/arm/v6. docker run -it --rm \\ --name nfs \\ --privileged \\ -v /tmp/volume:/nfs-share \\ -e SHARED_DIRECTORY=/nfs-share \\ -p 2049:2049 \\ itsthenetwork/nfs-server-alpine:latest  note In OSX, due to the docker desktop itself is running in VM, it will cause some error like Operation not supported when binding a local file folder via bind mount even you set 777 mask on the folder. So it's recommended to use volume to bind to /mnt in Samba server in OSX. Furthermore, the Samba server will log such message: error reading meta xattr: Not supported. Get the ip address of the NFS server, which will be used later to connect the NFS server when mounting in a Docker container client. If you only want to mount the NFS server from the host, you can just know the ip address of your host. docker inspect \\ -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' \\ nfs  Here the output is 172.17.0.2  "},{"title":"Use NFS Client in Docker​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/blog/docker-setup-nfs-sever#use-nfs-client-in-docker","content":"Let's make use of a Docker container to act in a NFS client to access shared data in the running-up NFS server. note run container as root by option --privileged or --cap-add SYS_ADMIN when permissions denied inside the container: docker run -it --rm --privileged busybox sh  Inside the container: note Due to the fsid=0 parameter set in the /etc/exports file, there's no need to specify the folder name when mounting from a client. For example, this works fine even though the folder being mounted and shared is /nfs-share: # In the container mkdir /mnt # nfs v4 mount -v -o vers=4,loud 172.17.0.2:/ /mnt # create a file to test echo &quot;some text here&quot; &gt; /mnt/file1.txt  Then go to the Host to list directory /data/volume/test, where you will find the file1.txt is sitting. # In the host cat /data/volume/test/file1.txt  "},{"title":"Use NFS Client With Volume Mount in Docker​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/blog/docker-setup-nfs-sever#use-nfs-client-with-volume-mount-in-docker","content":"Create a NFS volume in Docker docker volume create --driver local \\ --opt type=nfs \\ --opt o=addr=172.17.0.2,nfsvers=4 \\ --opt device=:/ \\ nfs-volume  docker inspect nfs-volume  Run the container with the created volume nfs-volume. docker run -it --rm \\ --privileged \\ --name nfs-test \\ -v nfs-volume:/mnt \\ busybox \\ sh  Alternative, you can use the combined one command which will create a volume nfsvolume, docker run -it --rm \\ --privileged \\ --name nfs-test \\ --mount 'type=volume,source=nfsvolume,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/,&quot;volume-opt=o=addr=172.17.0.2,rw,nfsvers=4,async&quot;,target=/mnt' \\ busybox \\ sh  "},{"title":"Setup a NFS Server and Mount NFS Volume int Docker Compose​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/blog/docker-setup-nfs-sever#setup-a-nfs-server-and-mount-nfs-volume-int-docker-compose","content":""},{"title":"About NFS Options​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/blog/docker-setup-nfs-sever#about-nfs-options","content":"Understanding the /etc/exports File – The Geek Diary "},{"title":"Docker Useful Commands","type":0,"sectionRef":"#","url":"/blog/docker-useful-commands","content":"","keywords":"Docker Useful Commands"},{"title":"Docker​","type":1,"pageTitle":"Docker Useful Commands","url":"/blog/docker-useful-commands#docker","content":"Famous busybox image that provide many common UNIX utilities for testing. docker run -it --rm --privileged busybox sh  Find the IP address of Docker container docker inspect \\ -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' \\ nfs  Delete all containers(include all status of running, stopped, created) docker rm -f $(docker ps -a -q)  Delete all volumes docker volume rm $(docker volume ls -q)  Keep container running for for testing and debugging # Use -t(-tty) docker run --rm -d -t busybox  docker run --rm -d busybox tail -f /dev/null  docker run --rm -d busybox sleep infinity  "},{"title":"Docker Compose​","type":1,"pageTitle":"Docker Useful Commands","url":"/blog/docker-useful-commands#docker-compose","content":"Rebuild image and restart a service which you specified, docker-compose up --no-deps web-app -d  Remove a service, docker-compose rm -s -v web-auth  "},{"title":"Dotfiles Guide","type":0,"sectionRef":"#","url":"/blog/dotfiles-guide","content":"Tutorials - dotfiles.github.io Getting started with dotfilesFrontend Ramblings RSS feedThe content of this website on GitHubMy Mastodon profileMy Twitter profileShare this article on TwitterShare this article on Hacker News","keywords":"dotfiles"},{"title":".NET Finalizer","type":0,"sectionRef":"#","url":"/blog/dotnet-finalizer","content":"","keywords":""},{"title":"Tips​","type":1,"pageTitle":".NET Finalizer","url":"/blog/dotnet-finalizer#tips","content":"finalizers should not be accessing managed objects. "},{"title":"FastAPI Best Practices","type":0,"sectionRef":"#","url":"/blog/fastapi-best-practices","content":"","keywords":"limit only one access to endpoint at a time lock access to endpoint at a time"},{"title":"Limit Only One Access to Endpoint at a Time​","type":1,"pageTitle":"FastAPI Best Practices","url":"/blog/fastapi-best-practices#limit-only-one-access-to-endpoint-at-a-time","content":"Limit only one access to an endpoint at a time with asyncio.Lock in asyncio in FastAPI. ../code-snippets/app_request_lock.py # main.py from fastapi import FastAPI import asyncio app = FastAPI() lock = asyncio.Lock() counter = 0 @app.post(&quot;/limit&quot;) async def func(): global counter async with lock: print(&quot;Hello&quot;) counter = counter + 1 await asyncio.sleep(2) print(&quot;bye&quot;) await asyncio.sleep(2) return {&quot;counter&quot;: counter} &quot;&quot;&quot; Make 2 requests at a time, output from server: INFO: 127.0.0.1:60228 - &quot;POST /limit HTTP/1.1&quot; 200 OK Hello bye INFO: 127.0.0.1:51010 - &quot;POST /limit HTTP/1.1&quot; 200 OK Hello bye INFO: 127.0.0.1:51022 - &quot;POST /limit HTTP/1.1&quot; 200 OK Request 1: ❯ curl -X 'POST' \\ 'http://127.0.0.1:8000/limit' \\ -H 'accept: application/json' \\ -d '' {&quot;counter&quot;:1}% Request 2: ❯ curl -X 'POST' \\ 'http://127.0.0.1:8000/limit' \\ -H 'accept: application/json' \\ -d '' {&quot;counter&quot;:2}% &quot;&quot;&quot;  NOTE: The asyncio.Lock only take effect in the asyncio loop level, if using unicorn to run server in multiple processes, it can not lock the request! No limitation. ../code-snippets/app_request_nolock.py # main.py from fastapi import FastAPI import asyncio app = FastAPI() lock = asyncio.Lock() counter = 0 @app.post(&quot;/limit&quot;) async def func(): global counter print(&quot;Hello&quot;) counter = counter + 1 await asyncio.sleep(2) print(&quot;bye&quot;) await asyncio.sleep(2) return {&quot;counter&quot;: counter} &quot;&quot;&quot; Make 2 requests at a time, output from server: Hello Hello bye bye INFO: 127.0.0.1:45160 - &quot;POST /limit HTTP/1.1&quot; 200 OK INFO: 127.0.0.1:45172 - &quot;POST /limit HTTP/1.1&quot; 200 OK Request 1: ❯ curl -X 'POST' \\ 'http://127.0.0.1:8000/limit' \\ -H 'accept: application/json' \\ -d '' {&quot;counter&quot;:2}% Request 2: ❯ curl -X 'POST' \\ 'http://127.0.0.1:8000/limit' \\ -H 'accept: application/json' \\ -d '' {&quot;counter&quot;:2}% &quot;&quot;&quot;  Limit only one access to an endpoint at a time with thread.Lock Limit only one access to an endpoint at a time with process.Lock "},{"title":"Attach A Background Service Into the Application​","type":1,"pageTitle":"FastAPI Best Practices","url":"/blog/fastapi-best-practices#attach-a-background-service-into-the-application","content":"Run a background service behind the FastAPI server: share the same asyncio main loop with the serverthe service start when the server starts and stop when the server stopsit should be light-weight and non-CPU heavy workload Coroutines and Tasks — Python 3.11.4 documentationEvent Loop — Python 3.11.4 documentation ../code-snippets/app_background_service.py from fastapi import FastAPI import asyncio import os app = FastAPI() class BackgroundService: def __init__(self, loop: asyncio.AbstractEventLoop, tasks: list): self.loop = loop self.running = False async def work(self): print(f&quot;Start background service&quot;) while True: print(f&quot;Run background service...&quot;) # Sleep for 1 second await asyncio.sleep(1) async def start(self): self.task = self.loop.create_task(self.work()) async def stop(self): self.task.cancel() try: await self.task except asyncio.CancelledError: print(&quot;Clean up background service&quot;) service = BackgroundService(asyncio.get_running_loop()) @app.on_event(&quot;startup&quot;) async def startup(): print(f&quot;PID[{os.getpid()}] app startup&quot;) # schedule a task on main loop await service.start() @app.on_event(&quot;shutdown&quot;) async def shutdown(): # close ProcessPoolExecutor print(f&quot;PID[{os.getpid()}] app shutdown&quot;) await service.stop() @app.post(&quot;/&quot;) async def hello(): return {&quot;value&quot;: f&quot;hello world [{service.task.done()}] [{service.task.get_name()}]&quot;}  "},{"title":"MDX Features of Docusaurus","type":0,"sectionRef":"#","url":"/blog/docusaurus-mdx-features","content":"","keywords":"mdx features in docusaurus"},{"title":"React Component​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/blog/docusaurus-mdx-features#react-component","content":"Code as: docs/mdx-features.mdx export const Highlight = ({children, color}) =&gt; ( &lt;span style={{ backgroundColor: color, borderRadius: '2px', color: '#fff', padding: '0.2rem', }}&gt; {children} &lt;/span&gt; ); &lt;Highlight color=&quot;#25c2a0&quot;&gt;Docusaurus green&lt;/Highlight&gt; and &lt;Highlight color=&quot;#1877F2&quot;&gt;Facebook blue&lt;/Highlight&gt; are my favorite colors.  Render as: Docusaurus green and Facebook blue are my favorite colors.    Code as: src/components/Highlight import React from 'react'; export default function SharedHighlight({children, color}) { return ( &lt;span style={{ backgroundColor: color, borderRadius: '2px', color: '#fff', padding: '0.2rem', }}&gt; {children} &lt;/span&gt; ); }  With: docs/mdx-features.mdx import SharedHighlight from '@site/src/components/Highlight'; &lt;SharedHighlight color=&quot;#25c2a0&quot;&gt;Docusaurus green&lt;/SharedHighlight&gt; I can write **Markdown** alongside my _JSX_!  Render as: Docusaurus green I can write Markdown alongside my JSX! "},{"title":"Tabs​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/blog/docusaurus-mdx-features#tabs","content":"Code as: docs/mdx-features.mdx import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; &lt;Tabs&gt; &lt;TabItem value=&quot;apple&quot; label=&quot;Apple&quot; default&gt; This is an apple 🍎 &lt;/TabItem&gt; &lt;TabItem value=&quot;orange&quot; label=&quot;Orange&quot;&gt; This is an orange 🍊 &lt;/TabItem&gt; &lt;TabItem value=&quot;banana&quot; label=&quot;Banana&quot;&gt; This is a banana 🍌 &lt;/TabItem&gt; &lt;/Tabs&gt;  Render as: AppleOrangeBanana This is an apple 🍎 "},{"title":"NOTES​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/blog/docusaurus-mdx-features#notes","content":"docs/mdx-features.mdx &lt;!-- Prettier doesn't change this --&gt; :::note Hello world :::  note Hello world &lt;!-- Prettier changes this --&gt; :::note Hello world :::  &lt;!-- to this --&gt; ::: note Hello world:::  docs/mdx-features.mdx :::note Your Title Some **content** with _Markdown_ `syntax`. :::  Your Title Some content with Markdown syntax. docs/mdx-features.mdx :::tip Use tabs in admonitions &lt;Tabs&gt; &lt;TabItem value=&quot;apple&quot; label=&quot;Apple&quot;&gt;This is an apple 🍎&lt;/TabItem&gt; &lt;TabItem value=&quot;orange&quot; label=&quot;Orange&quot;&gt;This is an orange 🍊&lt;/TabItem&gt; &lt;TabItem value=&quot;banana&quot; label=&quot;Banana&quot;&gt;This is a banana 🍌&lt;/TabItem&gt; &lt;/Tabs&gt; :::  Use tabs in admonitions AppleOrangeBanana This is an apple 🍎 "},{"title":"Math​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/blog/docusaurus-mdx-features#math","content":"docs/mdx-features.mdx Let $f\\colon[a,b]\\to\\R$ be Riemann integrable. Let $F\\colon[a,b]\\to\\R$ be $F(x)=\\int_{a}^{x} f(t)\\,dt$. Then $F$ is continuous, and at all $x$ such that $f$ is continuous at $x$, $F$ is differentiable at $x$ with $F'(x)=f(x)$.  Let f ⁣:[a,b]→Rf\\colon[a,b]\\to\\Rf:[a,b]→R be Riemann integrable. Let F ⁣:[a,b]→RF\\colon[a,b]\\to\\RF:[a,b]→R beF(x)=∫axf(t) dtF(x)=\\int_{a}^{x} f(t)\\,dtF(x)=∫ax​f(t)dt. Then FFF is continuous, and at all xxx such thatfff is continuous at xxx, FFF is differentiable at xxx with F′(x)=f(x)F'(x)=f(x)F′(x)=f(x). docs/mdx-features.mdx $$ I = \\int_0^{2\\pi} \\sin(x)\\,dx $$  I=∫02πsin⁡(x) dxI = \\int_0^{2\\pi} \\sin(x)\\,dxI=∫02π​sin(x)dx "},{"title":"Diagrams​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/blog/docusaurus-mdx-features#diagrams","content":"Example Mermaid diagram ```mermaid graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; ```  "},{"title":"Code Block​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/blog/docusaurus-mdx-features#code-block","content":"```jsx title=&quot;/src/components/HelloCodeTitle.js&quot; function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; } ```  /src/components/HelloCodeTitle.js function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; }  ```js function HighlightSomeText(highlight) { if (highlight) { // highlight-next-line return 'This text is highlighted!'; } return 'Nothing highlighted'; } function HighlightMoreText(highlight) { // highlight-start if (highlight) { return 'This range is highlighted!'; } // highlight-end return 'Nothing highlighted'; } ```  function HighlightSomeText(highlight) { if (highlight) { return 'This text is highlighted!'; } return 'Nothing highlighted'; } function HighlightMoreText(highlight) { if (highlight) { return 'This range is highlighted!'; } return 'Nothing highlighted'; }  ```jsx {1,4-6,11} import React from 'react'; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent; ```  import React from 'react'; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent;  ```jsx {1,4-6,11} showLineNumbers import React from 'react'; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent; ```  import React from 'react'; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent;  JavaScriptPythonJava function helloWorld() { console.log('Hello, world!'); }  "},{"title":"Importing Code​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/blog/docusaurus-mdx-features#importing-code","content":"npm install --save raw-loader  docs/mdx-features.mdx import CodeBlock from '@theme/CodeBlock'; import MyComponentSource from '!!raw-loader!./myComponent'; &lt;CodeBlock language=&quot;jsx&quot;&gt;{MyComponentSource}&lt;/CodeBlock&gt;  /** * Copyright (c) Facebook, Inc. and its affiliates. * * This source code is licensed under the MIT license found in the * LICENSE file in the root directory of this source tree. */ import React, { useState } from 'react'; export default function MyComponent() { const [bool, setBool] = useState(false); return ( &lt;div&gt; &lt;p&gt;MyComponent rendered !&lt;/p&gt; &lt;p&gt;bool={bool ? 'true' : 'false'}&lt;/p&gt; &lt;p&gt; &lt;button onClick={() =&gt; setBool((b) =&gt; !b)}&gt;toggle bool&lt;/button&gt; &lt;/p&gt; &lt;/div&gt; ); }  "},{"title":"Importing Markdown​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/blog/docusaurus-mdx-features#importing-markdown","content":"docs/mdx-features.mdx import PartialExample from './_markdown-partial-example.mdx'; &lt;PartialExample name=&quot;Sebastien&quot; /&gt;  Hello Sebastien This is text some content from _markdown-partial-example.mdx. docs/mdx-features.mdx import PartialExample1 from './skia.md'; &lt;PartialExample1 /&gt;  It imports file from skia.md: What the difference between SkImage/SkPicture/SkCanvas/SkSurface? SkBitmap based SkCanvas very slow... How to improve draw speeds? How to move SkImage from CPU to GPU? How to control the SkImage GPU back cache size? As far as I understand when I load SkImage from file or SkBitmap the SkImage lives in CPU side memory. Then the moment I draw this SkImage on a GPU backed canvas it will make a copy of the CPU data into a GPU backed texture. So now we technically have two copies available on the SkImage. Then each time I draw that SkImage it will do it quickly cause it's already in the GPU side. "},{"title":"Import Code Snippets from GitHub Repositories​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/blog/docusaurus-mdx-features#import-code-snippets-from-github-repositories","content":"A Docusaurus v2 plugin that supports referencing code examples from public GitHub repositories. src/theme/ReferenceCodeBlock/index.tsx loading... See full example on GitHub code-snippets/XKeyIn.cpp loading... See full example on GitHub "},{"title":"FastAPI Celery Serving ML Model","type":0,"sectionRef":"#","url":"/blog/fastapi-celery-serving-ml-model","content":"","keywords":"FastAPI Celery Serving ML Model"},{"title":"Celery Worker​","type":1,"pageTitle":"FastAPI Celery Serving ML Model","url":"/blog/fastapi-celery-serving-ml-model#celery-worker","content":"Celery Worker Mechanism: To start a Celery worker will start a main process that will spawn child processes or threads(based on the --pool option): the main process will handle receiving task/sending task result the and these child processes/threads(a.k.a execution pool) execute the actual tasks. To increase the number of child processes/threads(via --concurrency option) will increase the number of tasks the Celery worker can process in parallel. More processes are usually better. However, in reality, there are some situations in following modes: Run N workers with M child processes each.Run 1 worker with N*M child processes.Run N workers with only 1 main process each.Run N workers with M child threads each.Run 1 worker with N*M child threads. Whether to use processes or threads depends on what your tasks will actually do and whether they are GPU bound or IO bound. "},{"title":"Option --pool=prefork​","type":1,"pageTitle":"FastAPI Celery Serving ML Model","url":"/blog/fastapi-celery-serving-ml-model#option---poolprefork","content":"It spawns multiple processes. When start a Celery worker via celery -A tasks worker --loglevel INFO --concurrency 3 --pool=prefork, what will happen underneath? Celery start a main process.The main process will then spawn 3 child processes. The default concurrency is based on the number of CPU available on the machine. The default pool is prefork which uses multiprocessing library from Python.These child processes will execute the tasks assigned from the main process. "},{"title":"Option --pool=eventlet or --pool=gevent​","type":1,"pageTitle":"FastAPI Celery Serving ML Model","url":"/blog/fastapi-celery-serving-ml-model#option---pooleventlet-or---poolgevent","content":"It creates multiple threads. When start a Celery worker via celery -A tasks worker --loglevel INFO --concurrency 3 --pool=eventlet "},{"title":"Option --pool=solo​","type":1,"pageTitle":"FastAPI Celery Serving ML Model","url":"/blog/fastapi-celery-serving-ml-model#option---poolsolo","content":"It will not create any child process or thread to run task. The tasks will be executed in main process, which causes the main process to be blocked. It seems as: Run 1 worker with 1 process, however --concurrency will not take any effect when --pool=solo! When coming to a microservices environment, this option becomes useful and practical especially running CPU intensive tasks. The container manager such as Docker can increase the task processing capabilities through managing the number of worker containers instead of managing the number of pool processes per worker. When start a Celery worker via celery -A tasks worker --loglevel INFO --pool=solo "},{"title":"Celery Task​","type":1,"pageTitle":"FastAPI Celery Serving ML Model","url":"/blog/fastapi-celery-serving-ml-model#celery-task","content":""},{"title":"What is Task in Celery​","type":1,"pageTitle":"FastAPI Celery Serving ML Model","url":"/blog/fastapi-celery-serving-ml-model#what-is-task-in-celery","content":""},{"title":"How Task be Executed​","type":1,"pageTitle":"FastAPI Celery Serving ML Model","url":"/blog/fastapi-celery-serving-ml-model#how-task-be-executed","content":"from celery import Celery, Task app = Celery(...) @app.task(base=Task) def add(x, y): return x + y @app.task(base=Task) def mul(x, y): return x * y  The @app.task decoration will use Task class in default if you don't specify explicitly. When a worker start by celery -A tasks worker, Worker will spawn child Processes, the number of child Processes is based on CPU cores in default.Each child Process will initialize a Task instance for every decorated function. Here add() has its own Task instance and mul() also has its own Task instance respectively. When a client call add.delay(1, 2), Worker receive a Task in Queue.Worker assign the Task to a child Process, which will determine to use which Task instance to execute. A Task instance is initialized in each decorated function and registered with a task name using function name in default(such as add, mul). Here is the Task instance with name add() should be picked up to run the task.When be decorated in add(), the Task instance run() method will be add() original function body. The child Process will use the Task instance's __call__() method to run task, and __call__() will invoke the run() within itself.  "},{"title":"Known Issues​","type":1,"pageTitle":"FastAPI Celery Serving ML Model","url":"/blog/fastapi-celery-serving-ml-model#known-issues","content":"Result state is always PENDING in windows FIX: use --pool=solo instead of --pool=prefork in default. multiprocessing may cause this problem as its some defect in windows! "},{"title":"References​","type":1,"pageTitle":"FastAPI Celery Serving ML Model","url":"/blog/fastapi-celery-serving-ml-model#references","content":"Celery Execution Pools: What is it all about? Celery Exececution Pool: The worker and the pool - separation of concerns Serving ML Models in Production with FastAPI and Celery | by Jonathan Readshaw | Towards Data Science GitHub - liviaerxin/FastAPISpamDetection: Code for my Medium article: &quot;How you can quickly deploy your ML models with FastAPI&quot; "},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/blog/frequently-asked-questions","content":"","keywords":""},{"title":"Links/Graphs/BackLinks don't work. How do I enable them?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/blog/frequently-asked-questions#linksgraphsbacklinks-dont-work-how-do-i-enable-them","content":"Ensure that you have all the [[recommended-extensions]] installed in Visual Studio CodeReload Visual Studio Code by running Cmd + Shift + P (Ctrl + Shift + P for Windows), type &quot;reload&quot; and run the Developer: Reload Window command to for the updated extensions take effectCheck the formatting rules for links on [[foam-file-format]] and [[wikilinks]] "},{"title":"I don't want Foam enabled for all my workspaces​","type":1,"pageTitle":"Frequently Asked Questions","url":"/blog/frequently-asked-questions#i-dont-want-foam-enabled-for-all-my-workspaces","content":"Any extension you install in Visual Studio Code is enabled by default. Given the philosophy of Foam, it works out of the box without doing any configuration upfront. In case you want to disable Foam for a specific workspace, or disable Foam by default and enable it for specific workspaces, it is advised to follow the best practices as documented by Visual Studio Code "},{"title":"I want to publish the graph view to GitHub pages or Vercel​","type":1,"pageTitle":"Frequently Asked Questions","url":"/blog/frequently-asked-questions#i-want-to-publish-the-graph-view-to-github-pages-or-vercel","content":"If you want a different front-end look to your published foam and a way to see your graph view, we'd recommend checking out these templates: foam-gatsby by Mathieu Dutourfoam-gatsby-kb by hikerpig "},{"title":"FFmpeg Command Samples","type":0,"sectionRef":"#","url":"/blog/ffmpeg-command-samples","content":"","keywords":"learn ffmpeg"},{"title":"Overview​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#overview","content":"Examples of Container and Codec lists in Chromechrome: Video Container Format: MP4 [.mp4 file extension]OggWebMWAVHLS [.m3u8 file extension] Video Codec Format: VP8VP9H.264 [Chrome only]H.265 [Chrome only and also only with the underlying OS support]MPEG-4 [Chrome OS only, aka Xvid, DivX] "},{"title":"About FFmpeg Command​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#about-ffmpeg-command","content":"The common patter of ffmpeg looks like: ffmpeg [input_options] -i input.mp4 [output_options] output.mp4  In short: The [input_options] before -i input.mp4 are options used for decoding the videoThe [output_options] before output.mp4 are options used for encoding the video "},{"title":"FFmpeg Command Examples​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#ffmpeg-command-examples","content":"It's note worthing to look over FFmpeg Wiki ffmpeg "},{"title":"List all available container formats​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#list-all-available-container-formats","content":"ffmpeg -formats  "},{"title":"List all available codec formats​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#list-all-available-codec-formats","content":"ffmpeg -codecs  "},{"title":"List all available encoder or decoder​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#list-all-available-encoder-or-decoder","content":"ffmpeg -encoders ffmpeg -decoders  # Show available `presets` ffmpeg -h encoder=h264_nvenc  "},{"title":"List all frames timestamp​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#list-all-frames-timestamp","content":"ffprobe -select_streams v -show_entries frame=pict_type,pts_time -of csv=p=0 -i input.mp4  "},{"title":"List all keyframe(I-frame) timestamp​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#list-all-keyframei-frame-timestamp","content":"ffprobe -select_streams v -show_entries frame=pict_type,pts_time -of csv=p=0 -skip_frame nokey -i input.mp4  "},{"title":"Read video information json output​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#read-video-information-json-output","content":"ffprobe -v quiet -show_streams -select_streams v:0 -print_format json video.mp4  "},{"title":"Transcode video​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#transcode-video","content":"There are three possible and reasonable methods for transcoding: software decoding and software encodingsoftware decoding and hardware encodinghardware decoding and hardware encoding you can convert either the container formats or the codecs formats, such as: # To `mp4` container and `h.264` codecs(the lower crf, the higher quality) ffmpeg -i input.avi -c:v libx264 -preset fast -crf 23 output.mp4 # To `mp4` container and `mpeg4` codecs ffmpeg -i input.avi -c:v libxvid -preset fast output.mp4 # To be friendly for streaming, adding necessary metadata to begin playback faster! ffmpeg -i input.avi -c:v libx264 -preset fast -crf 23 -movflags +faststart output.mp4 # Remove audio ffmpeg -i input.avi -c:v libx264 -preset fast -crf 23 -an output.mp4 # Use NVIDIA GPU ffmpeg -i input.avi -c:v h264_nvenc -preset fast output.avi # keep quality ffmpeg -i input.avi -c:v h264_nvenc -preset fast -rc constqp -cq 19 output.avi  "},{"title":"Set keyframe interval​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#set-keyframe-interval","content":"# mpeg4 ffmpeg -i input.avi -vcodec libxvid -preset fast -g 10 -keyint_min 10 -sc_threshold 0 output.avi # NVIDIA GPU # This sets the I-frame interval at 10 and ensures that no I-frames will be inserted in scene changes ffmpeg -i input.avi -vcodec h264_nvenc -preset fast -g 10 -keyint_min 10 -sc_threshold 0 output.avi  "},{"title":"Clip video​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#clip-video","content":"# Fast clip with stream copy and faster seeking(700x) ffmpeg -ss 00:00:10 -i video.mp4 -to 00:00:50 -c:v copy output.mp4 # Fast clip with stream copy and slower seeking(600x) ffmpeg -i video.mp4 -ss 00:00:10 -to 00:00:50 -c:v copy output.mp4 # Slow clip with re-encoding and faster seeking(1x) ffmpeg -ss 00:00:10 -i video.mp4 -to 00:00:50 -c:v libx264 output.mp4  Use filter(Slow) ffmpeg -y -i input.mp4 -an -c:v libx264 -filter:v &quot;trim=start=10:end=30&quot; output.mp4 # remove the black video ffmpeg -y -i input.mp4 -an -c:v libx264 -filter:v &quot;trim=start=10:end=30,setpts=PTS-STARTPTS&quot; output.mp4  NOTE: Cutting video with stream copy will lead the start frame is not precise! "},{"title":"Slow down/Speed up video​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#slow-downspeed-up-video","content":"# Slow down to 1/2x in fast way ffmpeg -y -itsscale 2 -i video.mp4 -c:v copy output.mp4 # Speed up to 2x in fast way ffmpeg -y -itsscale 0.5 -i video.mp4 -c:v copy output.mp4 # Speed up to 2x with re-encoding in slow way ffmpeg -y -itsscale 0.5 -i video.mp4 -c:v libx264 output.mp4 # Speed up to 2x with `setpts filter`(which requires re-encoding) in slow way ffmpeg -i video.mp4 -filter:v &quot;setpts=0.5*PTS&quot; output.mp4 # Change fps to slow down/speed up but keeping duration ffmpeg -i video.mp4 -filter:v &quot;fps=30&quot; output.mp4  "},{"title":"Draw region of interest(ROI) on a video​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#draw-region-of-interestroi-on-a-video","content":"# Draw one drawbox ffmpeg -i input.mp4 -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red@0.5&quot; output.mp4 ffmpeg -i input.mp4 -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red@0.5,drawtext=text='Test Text':x=100:y=100:fontsize=24:fontcolor=yellow:box=1:boxcolor=yellow&quot; output.mp4 ffmpeg -y -ss 30 -noaccurate_seek -i input.mp4 -t 10 -c:v libx264 -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red@0.5,drawtext=text='Test Text':x=100:y=(100-text_h):fontsize=24:fontcolor=black:box=1:boxcolor=red:boxborderw=2&quot; output.mp4 # Trim video and draw box ffmpeg -y -i input.mp4 -an -c:v libx264 -filter:v &quot;trim=start=10:end=30,drawbox=x=100:y=100:w=200:h=150:color=red@0.5:enable='between(t,10,15)',setpts=PTS-STARTPTS&quot; output.mp4 ffmpeg -y -i input.mp4 -i overlay_video.mp4 -filter_complex &quot;[0:v][1:v]overlay=0:0:enable='between(t,0,25)'&quot; output.mp4  # Draw different `drawbox` at different time on video from a file(using `timeline` feature) # See timeline: https://ffmpeg.org/ffmpeg-filters.html#Timeline-editing # See expression: https://ffmpeg.org/ffmpeg-utils.html#Expression-Evaluation ffmpeg -i input.mp4 -filter_complex_script timeline.txt output.mp4 # `timeline.txt` look like: [0:v]drawbox=x=100:y=100:w=200:h=150:color=red@0.5:enable='between(t,0,21)'[box1]; [box1]drawbox=x=300:y=200:w=150:h=100:color=green@0.5:t=:enable='between(t,21,40)'[box2]; [box2]drawbox=x=50:y=300:w=300:h=200:color=blue@0.5:t=:enable='between(t,41,60)' # or using `n`: sequential number of the input frame, starting from 0 [0:v]drawbox=x=100:y=100:w=200:h=150:color=red@0.5:n=0:600[box1]; [box1]drawbox=x=300:y=200:w=150:h=100:color=green@0.5:n=601:1200[box2]; [box2]drawbox=x=50:y=300:w=300:h=200:color=blue@0.5:n=1201:1800  # Draw different `drawbox` at different time on video ffmpeg -i input.mp4 -filter_complex &quot;[0:v]drawbox=x=100:y=100:w=200:h=150:color=red:t=8:enable='between(t,0,21)'[box1];[box1]drawbox=x=300:y=200:w=150:h=100:color=green:t=8:enable='between(t,21,40)'[box2];[box2]drawbox=x=50:y=300:w=300:h=200:color=blue:t=8:enable='between(t,41,60)'&quot; output.mp4  "},{"title":"Pipe ffmpeg​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#pipe-ffmpeg","content":"There are a common use case for FFmpeg pipe: Another program will process images such as object detection, roi drawing but it lacks ability to generate a video efficiently. So it will pipe the image to FFmpeg which will encode a video by leveraging hardware acceleration(GPU). For example, pipe the sequence images output from a opencv process to the ffmpeg which produces the final video. # It works in Linux and Windows(`cmd`, does not work in `PS`) ffmpeg -ss 00:00:10 -i video.mp4 -to 00:00:20 -an -c:v copy -f h264 pipe: | ffmpeg -y -i pipe: -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red&quot; output.mp4 ffmpeg -i input.mp4 -c:v rawvideo -pix_fmt bgr24 -r 60 -f rawvideo pipe: | ffmpeg -y -f rawvideo -pix_fmt bgr24 -s 1920x1080 -r 60 -i pipe: -pix_fmt yuv420p -c:v h264_nvenc foo.mp4 ffmpeg -i input.mp4 -pix_fmt yuv420p -r 60 -f rawvideo pipe: | ffmpeg -y -f rawvideo -pix_fmt yuv420p -s 1920x1080 -r 60 -i pipe: -c:v h264_nvenc foo.mp4 ffmpeg -i input.mp4 -an -f h264 pipe: | ffmpeg -y -f h264 -i pipe: -c:v h264_nvenc foo.mp4  "},{"title":"Use testsrc​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#use-testsrc","content":"ffmpeg -y -f lavfi -i testsrc=duration=10:size=1920x1080:rate=60 -c:v libx264 -pix_fmt yuv420p testsrc.mp4  "},{"title":"Split and Concatenate​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#split-and-concatenate","content":"ffmpeg -y -i input.mp4 -ss 0 -to 10 -c:v copy part1.mp4 ffmpeg -y -i input.mp4 -ss 10 -to 15 -c:v copy part2.mp4 ffmpeg -y -i input.mp4 -ss 15 -c:v copy part3.mp4  ffmpeg -y -i part2.mp4 -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red@0.5&quot; part2-draw.mp4  Slow, ffmpeg -y -i part1.mp4 -i part2-draw.mp4 -i part3.mp4 -filter_complex &quot;[0:v][1:v][2:v]concat=n=3:v=1:a=0[outv]&quot; -map &quot;[outv]&quot; output.mp4  Fast(Concat protocol), ffmpeg -i part1.mp4 -c copy part1.ts ffmpeg -i part2-draw.mp4 -c copy part2-draw.ts ffmpeg -i part3.mp4 -c copy part3.ts ffmpeg -y -i &quot;concat:part1.ts|part2-draw.ts|part3.ts&quot; -c:v copy output.mp4  Fast(Concat demuxer), ffmpeg -y -f concat -i concat.txt -c:v copy output.mp4 # concat.txt file 'part1.mp4' file 'part2-draw.mp4' file 'part3.mp4' # Or avoid creating the input file # bash ffmpeg -y -f concat -safe 0 -i &lt;(echo &quot;file '$PWD/part1.mp4'&quot;;echo &quot;file '$PWD/part2-draw.mp4'&quot;;echo &quot;file '$PWD/part3.mp4'&quot;;) -c:v copy output.mp4 # cmd ffmpeg -y -f concat -safe 0 -i &lt;(@echo &quot;file '$PWD/part1.mp4'&quot;;@echo &quot;file '$PWD/part2-draw.mp4'&quot;;@echo &quot;file '$PWD/part3.mp4'&quot;;) -c:v copy output.mp4  https://trac.ffmpeg.org/wiki/Concatenate "},{"title":"References​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/blog/ffmpeg-command-samples#references","content":"Chrome Audio/Video Support↩FFmpeg Wiki↩ "},{"title":"Git Best Practices","type":0,"sectionRef":"#","url":"/blog/git-best-practices","content":"","keywords":"Git Submodules"},{"title":"Git SSH Key​","type":1,"pageTitle":"Git Best Practices","url":"/blog/git-best-practices#git-ssh-key","content":"How to Authenticate Your Git to GitHub with SSH Keys "},{"title":"Git Credentials​","type":1,"pageTitle":"Git Best Practices","url":"/blog/git-best-practices#git-credentials","content":"Store username/password instead of ssh for multiple remotes To enable git credentials # local git config credential.helper store # global git config --global credential.helper store  Each credential is stored in ~/.git-credentials file on its own line as a URL like: https://&lt;USERNAME&gt;:&lt;PASSWORD&gt;@github.com  Configure credentials, # Global git config --global credential.https://github.com.username &lt;your_username&gt; # Or git config --local user.name &lt;your_username&gt; git config --local user.email &lt;your_useremail&gt; # Then git pull or git push to let it cache your username/password after it prompt you to input password in the first time  Alternatively, we can directly edit our global Git config file ~/.gitconfig, [credential &quot;https://github.com&quot;] username = &lt;username&gt;  Git - Config Username &amp; Password - Store Credentials - ShellHacks Configuring Git Credentials "},{"title":"Git Submodules​","type":1,"pageTitle":"Git Best Practices","url":"/blog/git-best-practices#git-submodules","content":"Pull the repo and its all submodules in the first time. git clone http://10.6.64.66:30000/mtr/mtr.git cd mtr git submodule update --init --recursive --progress  Or just one command to clone with all the submodules. git clone --recursive http://10.6.64.66:30000/mtr/mtr.git  Pull the repo and its all submodules later git submodule update --recursive --progress  Enter each sub repository to pull its own latest of main per repository, when the parent repo does point to the latest branch of its submodules! Sometimes, it is very annoying to keep the parent repository up to date on the latest reference of its every sub repository! This approach give you the flexibility while being like a shortcut. git submodule foreach git checkout main  git submodule foreach git pull  git submodule foreach git pull origin main  git submodule update --recursive --remote  "},{"title":"Graphical User Interface(GUI)","type":0,"sectionRef":"#","url":"/blog/graphical-user-interface","content":"","keywords":""},{"title":"Android graphics​","type":1,"pageTitle":"Graphical User Interface(GUI)","url":"/blog/graphical-user-interface#android-graphics","content":"two core pieces: SurfaceFlingerSkia Graphics | Android Open Source ProjectAndroid Graphics Internals - Stack Overflow "},{"title":"WayLand​","type":1,"pageTitle":"Graphical User Interface(GUI)","url":"/blog/graphical-user-interface#wayland","content":"What is Wayland? · Writing Wayland clients The Hello Wayland Tutorial | FLOSS &amp; Cia How to use Wayland with C to make a Linux app | by Sergey Bugaev | Medium "},{"title":"IoC","type":0,"sectionRef":"#","url":"/blog/ioc","content":"Introduction IoC, DIP, DI and IoC Container Lazily resolving services to fix circular dependencies in .NET Core - Thomas Levesque's .NET Blog Dealing With Circular Dependency Injection References - .NET Core Tutorials","keywords":""},{"title":"Keyboard Shortcut Collection","type":0,"sectionRef":"#","url":"/blog/keyboard-shortcut-collection","content":"VS Code Keyboard Shortcuts Macos VS Code Keyboard Shortcuts Windows","keywords":"Keyboard Shortcut Collection"},{"title":"Learn ASGI","type":0,"sectionRef":"#","url":"/blog/learn-asgi","content":"Nowadays as web server framework is getting easy to use and work with. In Python areas, FastAPI obtains nearly 60k stars and becomes the most popular web framework for Pythoners. Looking at the advantage of FastAPI, it simplifies everything from parsing http requests, middleware processing, authentication, database manipulation and more. Let's dive into the behind-the-scenes technique stacks of FastAPI. Before research, there are some common questions around the web server development: How to process messages on HTTP protocol on TCP protocol? What're the favorite library used to do that?What are the differences between WSGI and ASGI?Data model used for database and users stacks from low-level to high-level Uvicorn: ASGI web server implementation/interface scopereceivesend h11 to process HTTP messageswebsocket to process websocket messages Starlette: ASGI frameworkabstract Request class for receive in Uvicornabstract Response class for send in Uvicornprovide middleware FastAPI: Fast to codeOpenAPI docsPydantic native model APIRoute APIRouter Application &lt;-- APIRouter &lt;-- APIRoute","keywords":"Learn ASGI"},{"title":"Inspect Shared Library","type":0,"sectionRef":"#","url":"/blog/inspect-shared-library","content":"","keywords":"debug dynamic library shared library"},{"title":"Using ldd Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#using-ldd-command","content":"Available in Linux: ldd /usr/bin/vim linux-vdso.so.1 (0x00007ffc75fb1000) libgtk-3.so.0 =&gt; /usr/lib/libgtk-3.so.0 (0x00007fa4dcb5e000) libgdk-3.so.0 =&gt; /usr/lib/libgdk-3.so.0 (0x00007fa4dca64000) libXau.so.6 =&gt; /usr/lib/libXau.so.6 (0x00007fa4db7a9000) .... liblzma.so.5 =&gt; /usr/lib/liblzma.so.5 (0x00007fa4db63f000) liblz4.so.1 =&gt; /usr/lib/liblz4.so.1 (0x00007fa4db61d000) libgcrypt.so.20 =&gt; /usr/lib/libgcrypt.so.20 (0x00007fa4db4ff000) libgpg-error.so.0 =&gt; /usr/lib/libgpg-error.so.0 (0x00007fa4db4d8000)  "},{"title":"Using objdump Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#using-objdump-command","content":"Available in Linux: objdump -p /usr/bin/vim | grep 'NEEDED' NEEDED libpython3.7m.so.1.0 NEEDED libcrypt.so.2 NEEDED libpthread.so.0 NEEDED libdl.so.2 NEEDED libutil.so.1 NEEDED libm.so.6 NEEDED libselinux.so.1 NEEDED libtinfo.so.6 NEEDED libacl.so.1 NEEDED libgpm.so.2 NEEDED libc.so.6  "},{"title":"Using readelf Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#using-readelf-command","content":"Available in Linux: readelf --dynamic /usr/bin/vim | grep NEEDED 0x0000000000000001 (NEEDED) Shared library: [libpython3.7m.so.1.0] 0x0000000000000001 (NEEDED) Shared library: [libcrypt.so.2] 0x0000000000000001 (NEEDED) Shared library: [libpthread.so.0] 0x0000000000000001 (NEEDED) Shared library: [libdl.so.2] 0x0000000000000001 (NEEDED) Shared library: [libutil.so.1] 0x0000000000000001 (NEEDED) Shared library: [libm.so.6] 0x0000000000000001 (NEEDED) Shared library: [libselinux.so.1] 0x0000000000000001 (NEEDED) Shared library: [libtinfo.so.6] 0x0000000000000001 (NEEDED) Shared library: [libacl.so.1] 0x0000000000000001 (NEEDED) Shared library: [libgpm.so.2] 0x0000000000000001 (NEEDED) Shared library: [libc.so.6]  "},{"title":"Using otool Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#using-otool-command","content":"Available in OSX: otool -L libOpenCvSharpExtern.dylib  "},{"title":"Reading the /proc/<pid>/maps File​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#reading-the-procpidmaps-file","content":"Available in Linux: cat /proc/179015/maps ... 7f2cb67c3000-7f2cb67c6000 r--p 00000000 08:13 3810274 /usr/lib/libnss_files-2.31.so 7f2cb67c6000-7f2cb67cd000 r-xp 00003000 08:13 3810274 /usr/lib/libnss_files-2.31.so .. 7f2cb6a89000-7f2cb6a8a000 r--p 00002000 08:13 3810903 /usr/lib/libutil-2.31.so 7f2cb6a8a000-7f2cb6a8b000 r--p 00002000 08:13 3810903 /usr/lib/libutil-2.31.so ... 7f2cb9802000-7f2cb9803000 rw-p 00000000 00:00 0 7ffe77658000-7ffe7767a000 rw-p 00000000 00:00 0 [stack] 7ffe776c8000-7ffe776cc000 r--p 00000000 00:00 0 [vvar] 7ffe776cc000-7ffe776ce000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall]  awk '$NF!~/\\.so/{next} {$0=$NF} !a[$0]++' /proc/179015/maps ... /usr/lib/libpython3.8.so.1.0 /usr/lib/libgpg-error.so.0.29.0 /usr/lib/libgcrypt.so.20.2.5 /usr/lib/liblz4.so.1.9.2 /usr/lib/liblzma.so.5.2.5 /usr/lib/libsystemd.so.0.28.0 /usr/lib/libogg.so.0.8.4 /usr/lib/libvorbis.so.0.4.8 /usr/lib/libblkid.so.1.1.0 /usr/lib/libXdmcp.so.6.0.0 /usr/lib/libXau.so.6.0.0 /usr/lib/libdatrie.so.1.3.5 ...  "},{"title":"Using vmmap Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#using-vmmap-command","content":""},{"title":"Using ctypes in Python​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#using-ctypes-in-python","content":"import ctypes ctypes.cdll.LoadLibrary(&quot;libOpenCvSharpExtern.so&quot;) ctypes.CDLL(&quot;libOpenCvSharpExtern.so&quot;)  dlopen() DYLD_PRINT_LIBRARIES=1 dlopen_test.out /opt/vcpkg/installed/arm64-osx-dynamic/lib/libpng16.dylib  objdump -p /usr/local/lib/libOpenCvSharpExtern.so  "},{"title":"Using nm​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#using-nm","content":"Show list of symbols: ❯ nm -g /opt/vcpkg/installed/arm64-osx-dynamic/lib/libintl.8.dylib U _CFArrayGetCount U _CFArrayGetValueAtIndex U _CFGetTypeID U _CFLocaleCopyPreferredLanguages U _CFPreferencesCopyAppValue U _CFRelease U _CFStringGetCString U _CFStringGetTypeID U __DefaultRuneLocale U ___CFConstantStringClassReference  "},{"title":"Using dumpbin​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#using-dumpbin","content":"Available in Windows Show dependent dynamic libraries(DLL): dumpbin /dependents your_dll_file.dll  "},{"title":"Using Microsoft.PowerShell​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#using-microsoftpowershell","content":"(Get-Command &quot;C:\\Path\\To\\Thing.dll&quot;).FileVersionInfo (Get-Item &quot;C:\\Windows\\System32\\nvcuda.dll&quot;).VersionInfo  "},{"title":"Useful Environment Variables​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#useful-environment-variables","content":"OSX: DYLD_LIBRARY_PATHDYLD_PRINT_LIBRARIESDYLD_PRINT_STATISTICS Linux: LD_LIBRARY_PATHLD_DEBUG=libs "},{"title":"References​","type":1,"pageTitle":"Inspect Shared Library","url":"/blog/inspect-shared-library#references","content":"Additional MSVC Build Tools How to Show All Shared Libraries Used by Executables in Linux "},{"title":"Learn CUDA","type":0,"sectionRef":"#","url":"/blog/learn-cuda","content":"","keywords":"gpu cuda"},{"title":"CUDA on WSL​","type":1,"pageTitle":"Learn CUDA","url":"/blog/learn-cuda#cuda-on-wsl","content":"NVIDIA CUDA software stack on WSL 2:  "},{"title":"Nvidia CUDA container​","type":1,"pageTitle":"Learn CUDA","url":"/blog/learn-cuda#nvidia-cuda-container","content":"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/user-guide.html "},{"title":"FFmpeg in Nvidia CUDA container​","type":1,"pageTitle":"Learn CUDA","url":"/blog/learn-cuda#ffmpeg-in-nvidia-cuda-container","content":"https://developer.nvidia.com/ffmpeg https://docs.nvidia.com/video-technologies/video-codec-sdk/12.0/ffmpeg-with-nvidia-gpu/index.html#compiling-for-linux https://developer.nvidia.com/blog/nvidia-ffmpeg-transcoding-guide/ "},{"title":"References​","type":1,"pageTitle":"Learn CUDA","url":"/blog/learn-cuda#references","content":"CUDA And Nvidia Graphics Driver CUDA on WSL User Guide "},{"title":"Learn CMake","type":0,"sectionRef":"#","url":"/blog/learn-cmake","content":"","keywords":"cmake project structure"},{"title":"CMake Project Structure​","type":1,"pageTitle":"Learn CMake","url":"/blog/learn-cmake#cmake-project-structure","content":"A typical CMake project can be regarded to has three Tree: Source Tree: project_root ├── CMakeLists.txt ├── simple_example.cpp ├── simple_lib.cpp └── simple_lib.hpp  Build Tree: project_root ├── CMakeLists.txt ├── simple_example.cpp ├── simple_lib.cpp ├── simple_lib.hpp └── build └── CMakeCache.txt  Install Tree: This tree is located in the CMAKE_INSTALL_PREFIX, of which default value is platform-dependent. By default, it is set to /usr/local on Unix-like systems (Linux, macOS) and C:/Program Files/&lt;Project Name&gt; on Windows.. To change it, you can pass -DCMAKE_INSTALL_PREFIX argument during CMake configuration step, like this: cmake -B build -S . -DCMAKE_INSTALL_PREFIX=/my/custom/installation/path  Alternatively, you can change it by passing --prefix(it can be relative path) argument during CMake install step, like this: cmake --install build --prefix &quot;/my/custom/installation/path&quot;  It's recommended to use a default install layout as GNUInstallDirs. When setting cmake --install build --prefix &quot;./install, the install tree will be like: project_root ├── CMakeLists.txt ├── simple_example.cpp ├── simple_lib.cpp ├── simple_lib.hpp ├── build │ └── CMakeCache.txt └── install ├── bin │ └── executables ├── sbin │ └── sysadmin executables ├── lib │ ├── compiled libraries (*.so (unix) or *.dll (windows)) │ └── library archive files (*.lib (windows)) ├── libexec │ └── executables not directly invoked by user ├── include │ └── header files └── doc └── documentation  "},{"title":"How CMake Works​","type":1,"pageTitle":"Learn CMake","url":"/blog/learn-cmake#how-cmake-works","content":"A typical workflow of CMake includes Configure, Build and Install steps, combined with the above mentioned Trees concepts. Configure step will generate a sort of configuration files, the most important ones among them are CMakeCache.txt, cmake_install.cmake and Makefile if using Make as building system. With these generated configuration files, the later steps Build and Install will run according to them.Build step will generate the build binary directory.Install step will generate the install binary directory. "},{"title":"How to make your package be found by others by find_package()​","type":1,"pageTitle":"Learn CMake","url":"/blog/learn-cmake#how-to-make-your-package-be-found-by-others-by-find_package","content":"package configuration files: find_package Title "},{"title":"RPATH in CMake​","type":1,"pageTitle":"Learn CMake","url":"/blog/learn-cmake#rpath-in-cmake","content":"rpath "},{"title":"CMake Variables​","type":1,"pageTitle":"Learn CMake","url":"/blog/learn-cmake#cmake-variables","content":"There are some useful and important CMake variables that will be introduced here: CMAKE_PREFIX_PATH CMAKE_IGNORE_PATH "},{"title":"References​","type":1,"pageTitle":"Learn CMake","url":"/blog/learn-cmake#references","content":"CMake hands-on workshop — CMake Workshoprpath: RPATH handling from official cmake "},{"title":"Learn PowerShell","type":0,"sectionRef":"#","url":"/blog/learn-powershell","content":"Set environment variables permanently [Environment]::SetEnvironmentVariable(&quot;VCPKG_ROOT&quot;, &quot;Whatever you need it to be&quot;, &quot;Machine&quot;) Get environment variables # Get all variables [Environment]::GetEnvironmentVariable() # Get specific variable [Environment]::GetEnvironmentVariable(&quot;VCPKG_ROOT&quot;) ","keywords":"learn powershell"},{"title":"LevelDB","type":0,"sectionRef":"#","url":"/blog/leveldb","content":"Principle and use of leveldb - Birost SSTable and Log Structured Storage: LevelDB - igvita.com LevelDB Benchmarks","keywords":""},{"title":"Learn Vcpkg","type":0,"sectionRef":"#","url":"/blog/learn-vcpkg","content":"","keywords":"docs docusaurus"},{"title":"Manifest Mode​","type":1,"pageTitle":"Learn Vcpkg","url":"/blog/learn-vcpkg#manifest-mode","content":""},{"title":"Classic Mode​","type":1,"pageTitle":"Learn Vcpkg","url":"/blog/learn-vcpkg#classic-mode","content":""},{"title":"Useful Environment variables​","type":1,"pageTitle":"Learn Vcpkg","url":"/blog/learn-vcpkg#useful-environment-variables","content":"CURRENT_INSTALLED_DIRCURRENT_PACKAGES_DIR set(VCPKG_RELEASE_LIBDIR &quot;${CURRENT_INSTALLED_DIR}/lib&quot;) set(VCPKG_DEBUG_LIBDIR &quot;${CURRENT_INSTALLED_DIR}/debug/lib&quot;) set(VCPKG_TOOLS_DIR &quot;${CURRENT_INSTALLED_DIR}/tools&quot;) set(VCPKG_SHARE_DIR &quot;${CURRENT_INSTALLED_DIR}/share&quot;) set(VCPKG_INCLUDE_DIR &quot;${CURRENT_INSTALLED_DIR}/include&quot;)  "},{"title":"Tips and Tricks​","type":1,"pageTitle":"Learn Vcpkg","url":"/blog/learn-vcpkg#tips-and-tricks","content":""},{"title":"Reinstall packages without caching​","type":1,"pageTitle":"Learn Vcpkg","url":"/blog/learn-vcpkg#reinstall-packages-without-caching","content":"vcpkg remove icu --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic vcpkg install icu --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic --no-binarycaching vcpkg install libpq --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic --binarysource=clear vcpkg remove libpq --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic vcpkg remove &quot;qtbase[gui,widgets]&quot; --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic vcpkg install &quot;qtbase[gui,widgets]&quot; --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic --no-binarycaching vcpkg install &quot;qtbase[gui,widgets]&quot; --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic --binarysource=clear  "},{"title":"Clean up all packages​","type":1,"pageTitle":"Learn Vcpkg","url":"/blog/learn-vcpkg#clean-up-all-packages","content":"rm -rf /opt/vcpkg/installed/ rm -rf /opt/vcpkg/packages/ rm -rf /opt/vcpkg/buildtrees/    "},{"title":"Clean up all caching packages​","type":1,"pageTitle":"Learn Vcpkg","url":"/blog/learn-vcpkg#clean-up-all-caching-packages","content":"rm -rf ~/.cache/vcpkg/archives/  "},{"title":"INSTALL_RPATH_USE_LINK_PATH different behaviours in manifest and classic mode​","type":1,"pageTitle":"Learn Vcpkg","url":"/blog/learn-vcpkg#install_rpath_use_link_path-different-behaviours-in-manifest-and-classic-mode","content":"INSTALL_RPATH_USE_LINK_PATH will not work properly when being used in the manifest mode, because CMake will don't handle libraries located in buildtree: set_target_properties(${PROJECT_NAME} PROPERTIES INSTALL_RPATH &quot;@executable_path/../Frameworks&quot; INSTALL_RPATH_USE_LINK_PATH ON )  After ${PROJECT_NAME} installed, in the manifest mode: ❯ otoolll /Users/frankchen/Documents/vcpkg-qt-app/install/./helloworld.app/Contents/MacOS/helloworld cmd LC_RPATH cmdsize 48 path @executable_path/../Frameworks (offset 12)  After ${PROJECT_NAME} installed, in the classic mode: ❯ otoolll /Users/frankchen/Documents/vcpkg-qt-app/install/./helloworld.app/Contents/MacOS/helloworld cmd LC_RPATH cmdsize 56 path /opt/vcpkg/installed/arm64-osx-dynamic/lib (offset 12) Load command 27 cmd LC_FUNCTION_STARTS -- cmd LC_RPATH cmdsize 48 path @executable_path/../Frameworks (offset 12)  [learn-cmake#RPATH in CMake] "},{"title":"Your markdown including PlantUML code block","type":0,"sectionRef":"#","url":"/blog/markdown-plantuml","content":"@startuml :User: --&gt; (Use) &quot;Main Admin&quot; as Admin &quot;Use the application&quot; as (Use) Admin --&gt; (Admin the application) @enduml ","keywords":""},{"title":"OpenCV tips","type":0,"sectionRef":"#","url":"/blog/opencv-tips","content":"Q: Whether the image/frame from VideoCapture is in BGR or YUV pixels format? A: VideoCapture will convert the image automatically to BGR colorspace. you can disable this conversion (and receive YUV) by setting the CAP_PROP_CONVERT_RGB property to false.","keywords":""},{"title":"Python Module","type":0,"sectionRef":"#","url":"/blog/py-module","content":"","keywords":""},{"title":"Python Module Search Path​","type":1,"pageTitle":"Python Module","url":"/blog/py-module#python-module-search-path","content":"The Module Search Path Introduction to Python module search path "},{"title":"Network Diagnosis","type":0,"sectionRef":"#","url":"/blog/network-diagnosis","content":"","keywords":""},{"title":"Show IP Address​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#show-ip-address","content":"ifconfig  "},{"title":"Show Routing Table​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#show-routing-table","content":"# linux route # osx netstat -rn  The -r flag means to show routes. The -n flag means to not resolve IPs to hostnames. "},{"title":"Find Gateway Used for Routing​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#find-gateway-used-for-routing","content":"# linux ip route get 8.8.8.8 # osx route get 8.8.8.8  "},{"title":"Show Routes across Network​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#show-routes-across-network","content":"traceroute # en0 interface traceroute -i en0  "},{"title":"Ping Through Specific Interface​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#ping-through-specific-interface","content":"# linux ping -I en0 sslvpn.astri.org # osx ping -b en0 sslvpn.astri.org  "},{"title":"Find Out Address Used by Which Process​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#find-out-address-used-by-which-process","content":"# osx netstat -avn -p tcp  "},{"title":"Add a Route​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#add-a-route","content":"# osx route -n add 10.0.0.0/24 10.0.0.1 # linux route -n add -net 10.0.0.0/24 gw 10.0.0.1  "},{"title":"FireWall Rule​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#firewall-rule","content":"osx: # show all information/stats sudo pfctl -sa # show rules sudo pfctl -sr # sanity check edited configuration file sudo pfctl -v -n -f /etc/pf.conf # load pf with new rules sudo pfctl -f /etc/pf.conf # enable pf sudo pfctl -e # disable pf sudo pfctl -d # add information on the fly sudo pfctl -t localsub -T add 127.0.0.0/24 # flush added rules later sudo pfctl -Fa -f /etc/pf.conf sudo pfctl -si sudo pfctl -q  "},{"title":"Get Geolocation of IP Address​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#get-geolocation-of-ip-address","content":"curl ipinfo.io/103.216.223.161  "},{"title":"Packet Analyzer​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#packet-analyzer","content":"# list which interfaces are available for capture tcpdump --list-interfaces # capture all packets in any interface sudo tcpdump --interface any # limit the number of packets captured then stop `-c number` sudo tcpdump -i any -c 5 # disable name resolution with using `-n` and port resolution with `-nn` sudo tcpdump -i any -c5 -nn # filter packets by protocol, only capture `ICMP` packets sudo tcpdump -i any -c5 icmp # capture packets related with host `8.8.8.8` sudo tcpdump -i any -c5 -nn host 8.8.8.8 # capture packets related with port `80` sudo tcpdump -i any -c5 -nn port 80 # capture packets with source address 192.168.0.1 sudo tcpdump -i any -c5 -nn src 192.168.0.1 # capture packets with destination address 8.8.8.8 sudo tcpdump -i any -c5 -nn dst 8.8.8.8  "},{"title":"USB Virtual Ethernet​","type":1,"pageTitle":"Network Diagnosis","url":"/blog/network-diagnosis#usb-virtual-ethernet","content":"An explanation on the USB virtual ethernet "},{"title":"Python Benchmark","type":0,"sectionRef":"#","url":"/blog/python-benchmark","content":"Ok, here is the cost of acquiring and releasing an uncontended lock under Linux, with Python 3.2: $ ./python -m timeit \\ -s &quot;from threading import Lock; l=Lock(); a=l.acquire; r=l.release&quot; \\ &quot;a(); r()&quot; 10000000 loops, best of 3: 0.127 usec per loop And here is the cost of calling a dummy Python function: $ ./python -m timeit -s &quot;def a(): pass&quot; &quot;a(); a()&quot; 1000000 loops, best of 3: 0.221 usec per loop And here is the cost of calling a trivial C function (which returns the False singleton): $ ./python -m timeit -s &quot;a=bool&quot; &quot;a(); a()&quot; 10000000 loops, best of 3: 0.164 usec per loop Also, note that using the lock as a context manager is actually slower, not faster as you might imagine: $ ./python -m timeit -s &quot;from threading import Lock; l=Lock()&quot; \\ &quot;with l: pass&quot; 1000000 loops, best of 3: 0.242 usec per loop At least under Linux, there doesn't seem to be a lot of room for improvement in lock performance, to say the least. PS: RLock is now as fast as Lock: $ ./python -m timeit \\ -s &quot;from threading import RLock; l=RLock(); a=l.acquire; r=l.release&quot; \\ &quot;a(); r()&quot; 10000000 loops, best of 3: 0.114 usec per loop ","keywords":""},{"title":"Python Package Management","type":0,"sectionRef":"#","url":"/blog/python-package-management","content":"","keywords":"docs docusaurus"},{"title":"Todo List​","type":1,"pageTitle":"Python Package Management","url":"/blog/python-package-management#todo-list","content":"First tabstopA second tabstopA third tabstop Note Created: 2023-06-26  Try out the above example by running the Foam: Create New Note From Template command and selecting the your-first-template template. Notice what happens when your new note is created! To remove this template, simply delete the .foam/templates/your-first-template.md file. Enjoy! "},{"title":"REST API Filtering, Sorting and Pagination","type":0,"sectionRef":"#","url":"/blog/rest-api-filtering-sorting-pagination","content":"api-guidelines/Guidelines.md at vNext · microsoft/api-guidelines · GitHub REST API Design: Filtering, Sorting, and Pagination | Moesif Blog How we write our query filter engine on our REST API (part 1) | by Adam Ben Aharon | Melio’s R&amp;D blog | Medium","keywords":"rest api filtering  sorting and pagination"},{"title":"Resumable Upload","type":0,"sectionRef":"#","url":"/blog/resumable-upload","content":"","keywords":"file upload resumable"},{"title":"A Basic Resumable Upload​","type":1,"pageTitle":"Resumable Upload","url":"/blog/resumable-upload#a-basic-resumable-upload","content":"code-snippets/app_resumable_upload.py loading... See full example on GitHub "},{"title":"TUS Resumable Upload​","type":1,"pageTitle":"Resumable Upload","url":"/blog/resumable-upload#tus-resumable-upload","content":"FastAPI implementing tus v1.0.0 server in Python code-snippets/app_tusd.py loading... See full example on GitHub Implementations | tus.io Resumable file upload GitHub - tus/tus-js-client: A pure JavaScript client for the tus resumable upload protocol GitHub - tus/tusd: Reference server implementation in Go of tus: the open protocol for resumable file uploads IO, StreamIO, FileIO high-level used by asyncio.io in socket/tcp/http Streams — Python 3.11.4 documentation starlette.Request.stream = http Request Body low-level: io — Core tools for working with streams — Python 3.11.4 documentation "},{"title":"RPC vs MQ","type":0,"sectionRef":"#","url":"/blog/rpc_vs_mq","content":"","keywords":""},{"title":"RPC​","type":1,"pageTitle":"RPC vs MQ","url":"/blog/rpc_vs_mq#rpc","content":""},{"title":"IPC​","type":1,"pageTitle":"RPC vs MQ","url":"/blog/rpc_vs_mq#ipc","content":"IPC: (local)Inter-Process Communication Using gRPC for (local) inter-process communication IPC Benchmark "},{"title":"MQ​","type":1,"pageTitle":"RPC vs MQ","url":"/blog/rpc_vs_mq#mq","content":""},{"title":"Skia","type":0,"sectionRef":"#","url":"/blog/skia","content":"What the difference between SkImage/SkPicture/SkCanvas/SkSurface? SkBitmap based SkCanvas very slow... How to improve draw speeds? How to move SkImage from CPU to GPU? How to control the SkImage GPU back cache size? As far as I understand when I load SkImage from file or SkBitmap the SkImage lives in CPU side memory. Then the moment I draw this SkImage on a GPU backed canvas it will make a copy of the CPU data into a GPU backed texture. So now we technically have two copies available on the SkImage. Then each time I draw that SkImage it will do it quickly cause it's already in the GPU side.","keywords":""},{"title":"Serialization","type":0,"sectionRef":"#","url":"/blog/serialization","content":"","keywords":"MessagePack msgpack json serialization Protocol Buffers Protobuf"},{"title":"Json​","type":1,"pageTitle":"Serialization","url":"/blog/serialization#json","content":""},{"title":"MessagePack​","type":1,"pageTitle":"Serialization","url":"/blog/serialization#messagepack","content":"msgpack GitHub "},{"title":"Protocol Buffers​","type":1,"pageTitle":"Serialization","url":"/blog/serialization#protocol-buffers","content":"Protocol Buffers "},{"title":"Supported Features​","type":1,"pageTitle":"Serialization","url":"/blog/serialization#supported-features","content":"Protocol\tDiscriminator Property &amp; PolymorphismJson\t✔️ MessagePack\t✔️ Protobuf\t✖️ "},{"title":"RAID on Ubuntu","type":0,"sectionRef":"#","url":"/blog/ubuntu-raid","content":"","keywords":"Setup Intel VROC RAID on Ubuntu"},{"title":"Background​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#background","content":"Recently, there is a chance for me to install Ubuntu(server) OS on a Dell Precision xxx workstation which is includes 2 NVMe SSDs and 8 SATA hard drives(HDDs) and has in-box hardware-assisted RAID controller(Intel VROC). In the past, I just play cloud virtual machines or personal host with single disk. Storage is an important consideration coming up to my mind. So firstly, how to organize these disks to their roles: 2 SSDs hold the system to load quickly8 HDDs store data persistently In order to access these physical disks easily and reduce damages from data loss, I need to combine multiple disks to act as one, while keep data redundant and backup. After step-by-step research, there are some enterprise solutions present for me. These drive layer or file system layer approaches designed for specific purposes have their own advantages over others while they maybe achieve some same features. Here are some benefits and shortcomings of them alongside common use cases: RAID(Redundant Array of Independent Disks) Abstraction level: drive layer Concept: RAID uses multiple drives to act as one(logical drive). Benefits: improve data redundancy and data read/write performance LVM(Logical Volume Management) Abstraction level: file system layer Concept: Manage a logical volume over multiple drives, each drive is a Physical Volume(PV). Benefits: combine multiple disks into one logical volume, extend the volume with new disk added, increase/decrease mounted folder in file system ZFS(Z File System) There are three types of raid, as Wiki saying: hardware RAIDsoftware RAID mdadm in Linux hardware-assisted software RAID, firmware RAID, fake RAID Intel VROC (Virtual RAID on CPU) This document will introduce how to set up software RAID(RAID0, RAID1, RAID5, RAID 10) on already-installed Ubuntu. To create a RAID to hold the Ubuntu OS when installing Ubuntu, see SoftwareRAID or How to install Ubuntu with software RAID-1 In addition, there are different challenges you will face when installing Ubuntu Server and Ubuntu Desktop. Install Ubuntu Server on RAID: Ubuntu Server Image has inbox mdadm utilities, so it is quite convenient to create the software RAID on multiple disks then install Ubuntu Server OS on the RAID in storage layer step during OS installation stage.  Install Ubuntu Desktop on RAID: Ubuntu Desk Image does not ship the mdadm tool, so it is nearly impossible to create RAID and install Ubuntu Desktop OS on the RAID(however this one Install Ubuntu 20.04 desktop with RAID 1 and LVM on machine with UEFI BIOS from stackoverflow seems to be successful) "},{"title":"Set up RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#set-up-raid-array","content":"To create a RAID array ready to use in practice, there are always common steps: Create a RAID array(RAID 0, RAID 1, RAID 5 or RAID 10)Mount the RAID arraySave the RAID array configuration for system boot "},{"title":"Create RAID array with mdadm​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#create-raid-array-with-mdadm","content":"Create RAID 0 array using devices: /dev/sda and /dev/sdb sudo mdadm --create --verbose /dev/md0 -l 0 -n 2 /dev/sda /dev/sdb  "},{"title":"Mount RAID array for use​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#mount-raid-array-for-use","content":"Create a ext4 filesystem on the array sudo mkfs.ext4 -F /dev/md0  Mount the array sudo mkdir -p /mnt/md0 sudo mount /dev/md0 /mnt/md0  "},{"title":"Save RAID array configuration​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#save-raid-array-configuration","content":"Persist the RAID array configuration to make the system reassemble and mount the RAID array automatically after reboot. Edit /etc/mdadm/mdadm.conf: /etc/mdadm/mdadm.conf sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf  Make RAID array available in early boot stage: sudo update-initramfs -u  Edit /etc/fstab: /etc/fstab echo '/dev/md0 /mnt/md0 ext4 defaults,nofail,discard 0 0' | sudo tee -a /etc/fstab  "},{"title":"Delete RAID Array with mdadm​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#delete-raid-array-with-mdadm","content":"Make sure to remove what are using the RAID array, [Optional] Umount the array from filesystem if mounted, sudo umount /dev/md0  Stop RAID array, sudo mdadm --stop /dev/md0 # Stop all arrays sudo mdadm --stop --scan  Removes the RAID metadata and resets them to normal on the Drives, sudo mdadm --zero-superblock /dev/sda sudo mdadm --zero-superblock /dev/sd[a-h]  [Optional] Remove any persistent references to the array if exist. Edit the /etc/fstab: /etc/fstab sudo nano /etc/fstab  [Optional] Also, remove the array definition if exist, from the /etc/mdadm/mdadm.conf file: /etc/mdadm/mdadm.conf sudo nano /etc/mdadm/mdadm.conf  "},{"title":"Manage RAID Array with mdadm​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#manage-raid-array-with-mdadm","content":""},{"title":"Finding the RAID arrays​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#finding-the-raid-arrays","content":"$ cat /proc/mdstat Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] md126 : active raid1 nvme0n1[1] nvme1n1[0] 3800741888 blocks super external:/md127/0 [2/2] [UU] md127 : inactive nvme0n1[1](S) nvme1n1[0](S) 10402 blocks super external:imsm unused devices: &lt;none&gt;  "},{"title":"Querying information on RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#querying-information-on-raid-array","content":"sudo mdadm --detail /dev/md0 sudo mdadm --query /dev/md0  "},{"title":"Getting information on individual physical devices​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#getting-information-on-individual-physical-devices","content":"sudo mdadm --query /dev/sda sudo mdadm --examine /dev/sda  "},{"title":"Stop RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#stop-raid-array","content":"sudo mdadm --stop /dev/md0 # Stop all arrays sudo mdadm --stop --scan  "},{"title":"Starting an RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#starting-an-raid-array","content":"# This works if the array is defined in the configuration `/etc/mdadm/mdadm.conf` file. sudo mdadm --assemble --scan sudo mdadm --assemble /dev/md0 # If the array is not persisted in `/etc/mdadm/mdadm.conf` file but keeping RAID metadata sudo mdadm --assemble /dev/md0 /dev/sda /dev/sdb  "},{"title":"Adding a spare device to an RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#adding-a-spare-device-to-an-raid-array","content":"sudo mdadm /dev/md0 --add /dev/sde  "},{"title":"Checking devices​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#checking-devices","content":"$ lsblk -f NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTS loop0 squashfs 4.0 0 100% /snap/core20/1974 loop1 squashfs 4.0 0 100% /snap/lxd/24322 loop2 squashfs 4.0 0 100% /snap/snapd/19457 sda isw_raid_member 1.3.00 sdb isw_raid_member 1.3.00 sdc isw_raid_member 1.3.00 sdd isw_raid_member 1.3.00 sde isw_raid_member 1.3.00 sdf isw_raid_member 1.3.00 sdg isw_raid_member 1.3.00 sdh isw_raid_member 1.3.00 nvme0n1 isw_raid_member 1.3.00 ├─md126 │ ├─md126p1 vfat FAT32 292B-DB66 1G 1% /boot/efi │ └─md126p2 ext4 1.0 0f58386c-334d-4877-8051-b855bae37fb0 3.3T 0% / └─md127 nvme1n1 isw_raid_member 1.3.00 ├─md126 │ ├─md126p1 vfat FAT32 292B-DB66 1G 1% /boot/efi │ └─md126p2 ext4 1.0 0f58386c-334d-4877-8051-b855bae37fb0 3.3T 0% / └─md127  "},{"title":"Partitioning a disk​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#partitioning-a-disk","content":"sudo fdisk -l /dev/sda  "},{"title":"Creating file system​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#creating-file-system","content":"sudo mkfs.ext4 -F /dev/sda  "},{"title":"Deleting partition and data in disk​","type":1,"pageTitle":"RAID on Ubuntu","url":"/blog/ubuntu-raid#deleting-partition-and-data-in-disk","content":"sudo dd if=/dev/zero of=/dev/sda bs=512 count=1  "},{"title":"VPN","type":0,"sectionRef":"#","url":"/blog/vpn","content":"","keywords":"vpn"},{"title":"Routers Support for VPN(OpenVPN) Client​","type":1,"pageTitle":"VPN","url":"/blog/vpn#routers-support-for-vpnopenvpn-client","content":"How to set up a router with Surfshark? – Surfshark Customer Support Routers Supporting VPN Client - Home Network Community "},{"title":"Kill Switch​","type":1,"pageTitle":"VPN","url":"/blog/vpn#kill-switch","content":"KillSwitch could be used to block outgoing traffic when the VPN connection drops and crashes. "},{"title":"PF(packet filter) MacOS​","type":1,"pageTitle":"VPN","url":"/blog/vpn#pfpacket-filter-macos","content":"Setting up correctly Packet Filter (pf) firewall on any macOS Prevent outgoing traffic unless OpenVPN connection is active using pf.conf on Mac OS X Quick and easy pf (packet filter) firewall rules on macOS A Cheat Sheet For Using pf in OS X Lion and Up OS X PF Manual "},{"title":"Set Up Firewall to Allow Access Only via VPN(KillSwitch)​","type":1,"pageTitle":"VPN","url":"/blog/vpn#set-up-firewall-to-allow-access-only-via-vpnkillswitch","content":"ENABLING VPN-ONLY ACCESS TO THE INTERNET WITH WINDOWS FIREWALL KillSwitch for macOS Prevent outgoing traffic unless OpenVPN connection is active using pf.conf on Mac OS X "},{"title":"Intel VROC RAID on Ubuntu","type":0,"sectionRef":"#","url":"/blog/ubuntu-intel-vroc-raid","content":"","keywords":"Setup Intel VROC RAID on Ubuntu"},{"title":"Background​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#background","content":"On the premise machine, there are 2 NVMe SSDs and 8 SATA hard drives(HDDs), and also it ships with a in-box hardware-assisted RAID controller(Intel VROC) on the Intel CPU, which is supposed to keep overall advantages over software RAID. For me, I would like to use these 8 HDDs(sda, sdb, ..., sdh) to store data for long time, while retaining the balance between redundancy and performance. So here RAID 5(Stripping with Parity) comes into my mind. To leverage the power of Intel VROC in Ubuntu(Linux), you also need the mdadm command line tool to manage intel VROC which support RAID 0, RAID 1, RAID 5 and RAID 10 note In my understanding, the intel VROC register in system with the common interface with mdadm, so the mdadm software can operate it. And running command will show the mdadm is using intel VROC, $ sudo mdadm --detail-platform Platform : Intel(R) Virtual RAID on CPU Version : 8.0.3.1002 RAID Levels : raid0 raid1 raid10 raid5 Chunk Sizes : 4k 8k 16k 32k 64k 128k 2TB volumes : supported 2TB disks : supported Max Disks : 8 Max Volumes : 2 per array, 8 per controller I/O Controller : /sys/devices/pci0000:00/0000:00:17.0 (SATA) Port7 : /dev/sdh (ZR909K07) Port6 : /dev/sdg (ZV70BN24) Port3 : /dev/sdd (ZV70BD3T) Port4 : /dev/sde (ZR909Q89) Port1 : /dev/sdb (ZRT0S2FM) Port5 : /dev/sdf (ZR9099MM) Port2 : /dev/sdc (ZR909AGN) Port0 : /dev/sda (ZV70BMH9) Platform : Intel(R) Virtual RAID on CPU Version : 8.0.3.1002 RAID Levels : raid0 raid1 raid10 Chunk Sizes : 4k 8k 16k 32k 64k 128k 2TB volumes : supported 2TB disks : supported Max Disks : 96 Max Volumes : 2 per array, 24 per controller 3rd party NVMe : supported I/O Controller : /sys/devices/pci0000:8d/0000:8d:00.5 (VMD) NVMe under VMD : /dev/nvme0n1 (633FC084FCVK) NVMe under VMD : /dev/nvme1n1 (633FC0DEFCVK) I/O Controller : /sys/devices/pci0000:6f/0000:6f:00.5 (VMD) I/O Controller : /sys/devices/pci0000:51/0000:51:00.5 (VMD)  info Install Ubuntu Server on RAID: Ubuntu Server Image has inbox mdadm utilities and VMD drivers(which enable intel VROC functionalities), so it is quite convenient to create the RAID 1 on 2 SSDs either in BIOS stage(for intel VROC only) or in storage layer step during OS installation stage(software RAID), then install Ubuntu Server OS on the RAID 1. After creating the RAID 1 via intel VROC in BIOS, Ubuntu Server installation can detect the RAID created by VROC in step when set up the storage layer. If you skip BIOS to create RAID during OS installation, remember to add -e isms when using mdadm to create RAID(you can enter the terminal, do ``) otherwise the RAID is software based and does not apply VROC. Install Ubuntu Desktop on RAID: Ubuntu Desk Image does not ship the mdadm tool, so it is nearly impossible to create RAID and install Ubuntu Desktop OS on the RAID(however this one Install Ubuntu 20.04 desktop with RAID 1 and LVM on machine with UEFI BIOS from stackoverflow seems to be successful) "},{"title":"Set up RAID 5 array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#set-up-raid-5-array","content":"Here, I use 8 disks: /dev/sda, /dev/sdb, ..., /dev/sdh to create RAID 5 array and mount it for use in practice. "},{"title":"Create RAID array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#create-raid-array","content":"When creating RAID array, Intel VROC is different with software RAID array creation as an additional container is needed to create firstly. Inside the container, some information is labelled into the drives for Intel VROC controller to recognize them. Create RAID Container with Intel IMSM Metadata the total number of drives is 8 and -e imsm. sudo mdadm --create /dev/md/imsm0 /dev/sd[a-h] -n 8 -e imsm  Then, Create a RAID array in the /dev/md/imsm0 container using total 8 drives with RAID 5. sudo mdadm --create /dev/md/md0 /dev/md/imsm0 -l 0 -n 2  "},{"title":"Mount the RAID array for use​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#mount-the-raid-array-for-use","content":"After you create the RAID array in above step, all partitions and data will be erased from all individual disks. The RAID array is treated as a logical drive now. Create a ext4 filesystem on the RAID array sudo mkfs.ext4 -F /dev/md/md0  Mount the RAID array sudo mkdir -p /mnt/md0 sudo mount /dev/md/md0 /mnt/md0  "},{"title":"Save RAID array configuration​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#save-raid-array-configuration","content":"To make sure that the RAID array is reassembled and mounted automatically after reboot, we will have to add some necessary information into /etc/mdadm/mdadm.conf and /etc/fstab. Scan active array and append into /etc/mdadm/mdadm.conf file with following: sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf  Update initramfs, so the array will be available at early boot: sudo update-initramfs -u  Add mount options to /etc/fstab, you can use UUID=xxxx instead of the /dev/md0. echo '/dev/md0 /mnt/md0 ext4 defaults,nofail,discard 0 0' | sudo tee -a /etc/fstab  "},{"title":"Remove RAID Array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#remove-raid-array","content":""},{"title":"[Optional] Umount the array from filesystem​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#optional-umount-the-array-from-filesystem","content":"Umount the array from filesystem if mounted, sudo umount /dev/md/md0  "},{"title":"Stop RAID container and array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#stop-raid-container-and-array","content":"# Stop RAID container sudo mdadm --stop /dev/md/imsm0 # Stop RAID array sudo mdadm --stop /dev/md/md0 # Stop all arrays and containers sudo mdadm --stop --scan  "},{"title":"Removes the RAID metadata​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#removes-the-raid-metadata","content":"Removes the RAID metadata on each drive and resets the drive to normal sudo mdadm --zero-superblock /dev/sda sudo mdadm --zero-superblock /dev/sd[a-h]  "},{"title":"[Optional] Remove RAID configuration​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#optional-remove-raid-configuration","content":"Remove mount information to the array if exist. Edit the /etc/fstab: /etc/fstab sudo nano /etc/fstab  Also, remove the array definition if exist, from the /etc/mdadm/mdadm.conf file: /etc/mdadm/mdadm.conf sudo nano /etc/mdadm/mdadm.conf  "},{"title":"Manage RAID Array with mdadm​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/blog/ubuntu-intel-vroc-raid#manage-raid-array-with-mdadm","content":"Finding the RAID arrays, $ cat /proc/mdstat Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] md126 : active raid1 nvme0n1[1] nvme1n1[0] 3800741888 blocks super external:/md127/0 [2/2] [UU] md127 : inactive nvme0n1[1](S) nvme1n1[0](S) 10402 blocks super external:imsm unused devices: &lt;none&gt;  Querying for Information on RAID array, sudo mdadm --detail /dev/md0 sudo mdadm --query /dev/md0  Getting Information on individual physical Devices, sudo mdadm --query /dev/sda sudo mdadm --examine /dev/sda  Stop RAID array, sudo mdadm --stop /dev/md0 # Stop all arrays sudo mdadm --stop --scan  Starting an Array, # This works if the array is defined in the configuration `/etc/mdadm/mdadm.conf` file. sudo mdadm --assemble --scan sudo mdadm --assemble /dev/md0 # If the array is not persisted in `/etc/mdadm/mdadm.conf` file but keeping RAID metadata sudo mdadm --assemble /dev/md0 /dev/sda /dev/sdb  Adding a Spare Device to an Array, sudo mdadm /dev/md0 --add /dev/sde  $ lsblk -f NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTS loop0 squashfs 4.0 0 100% /snap/core20/1974 loop1 squashfs 4.0 0 100% /snap/lxd/24322 loop2 squashfs 4.0 0 100% /snap/snapd/19457 sda isw_raid_member 1.3.00 sdb isw_raid_member 1.3.00 sdc isw_raid_member 1.3.00 sdd isw_raid_member 1.3.00 sde isw_raid_member 1.3.00 sdf isw_raid_member 1.3.00 sdg isw_raid_member 1.3.00 sdh isw_raid_member 1.3.00 nvme0n1 isw_raid_member 1.3.00 ├─md126 │ ├─md126p1 vfat FAT32 292B-DB66 1G 1% /boot/efi │ └─md126p2 ext4 1.0 0f58386c-334d-4877-8051-b855bae37fb0 3.3T 0% / └─md127 nvme1n1 isw_raid_member 1.3.00 ├─md126 │ ├─md126p1 vfat FAT32 292B-DB66 1G 1% /boot/efi │ └─md126p2 ext4 1.0 0f58386c-334d-4877-8051-b855bae37fb0 3.3T 0% / └─md127  sudo fdisk -l /dev/sda  Delete partition and data in drive, sudo dd if=/dev/zero of=/dev/sda bs=512 count=1  "},{"title":"WiFi AutoSwitch Windows","type":0,"sectionRef":"#","url":"/blog/wifi-autoswitch-windows","content":"If autoSwitch is turned on, it allows Windows to continue looking for other auto-connected wireless networks while connected to the current wireless network. If a higher priority auto-connected wireless network than the currently connected wireless network comes in range, Windows will automatically connect to it instead. It also needs to work along with priority setting. For example: There're 3 networks of profile name: TP-Link-1, TP-Link-2 and TP-Link-3. PC(windows) will try to connect TP-Link-3 if it's in range when it's already connected to TP-Link-1 or TP-Link-2. Setup autoswitch: netsh wlan set profileparameter name=&quot;TP-Link-1&quot; autoswitch=Yes netsh wlan set profileparameter name=&quot;TP-Link-2&quot; autoswitch=Yes netsh wlan set profileparameter name=&quot;TP-Link-3&quot; autoswitch=No Setup priority: netsh wlan set profileorder name=&quot;TP-Link-1&quot; interface=&quot;Wi-Fi&quot; priority=3 netsh wlan set profileorder name=&quot;TP-Link-2&quot; interface=&quot;Wi-Fi&quot; priority=2 netsh wlan set profileorder name=&quot;TP-Link-3&quot; interface=&quot;Wi-Fi&quot; priority=1 other tools: List profile name: netsh wlan show profiles List connected WiFi: netsh wlan show interfaces Enable Auto Switch for Wireless Network Connection in Windows 10 Change WiFi network priority in Windows 10","keywords":"autoswitch wifi windows"},{"title":"WPF","type":0,"sectionRef":"#","url":"/blog/wpf","content":"","keywords":""},{"title":"MVVM​","type":1,"pageTitle":"WPF","url":"/blog/wpf#mvvm","content":"MVVM Pattern Made Simple - CodeProject MVVM in Depth - CodeProject My attempt to understand MVVM pattern and questions raised during it : csharp Patterns - WPF Apps With The Model-View-ViewModel Design Pattern | Microsoft Docs Introduction to the MVVM Toolkit - Windows Community Toolkit | Microsoft Docs "},{"title":"Features​","type":1,"pageTitle":"WPF","url":"/blog/wpf#features","content":"IoC, Inversion of Control DI, Dependency Injection Navigation ViewModel-to-ViewModel Communication MVVM Light MessengerEvent Aggregator | PrismReactiveUI - Message Bus Observable Object in ViewModel Wrapping a non-observable model // https://docs.microsoft.com/en-us/windows/communitytoolkit/mvvm/observableobject#wrapping-a-non-observable-model public class ObservableUser : ObservableObject { private readonly User user; public ObservableUser(User user) =&gt; this.user = user; public string Name { get =&gt; user.Name; set =&gt; SetProperty(user.Name, value, user, (u, n) =&gt; u.Name = n); } }  "},{"title":"Principles​","type":1,"pageTitle":"WPF","url":"/blog/wpf#principles","content":" View-to-ViewModel one-to-one/many-to-one mappingViewModel-to-ViewModel communicationViewModel-to-Model one-to-one/one-to-many binding "},{"title":"Access Database​","type":1,"pageTitle":"WPF","url":"/blog/wpf#access-database","content":"DAO or Repository Entity DB Context "},{"title":"ReactiveUI​","type":1,"pageTitle":"WPF","url":"/blog/wpf#reactiveui","content":"To property - pasoft-share/ReactiveUI One of the core features of ReactiveUI is to be able to convert properties to Observables, via WhenAny , and to convert Observables into Properties, via a method called ToProperty . These properties are called Output Properties in ReactiveUI, and they are a huge part of using the framework effectively. "},{"title":"Monday, July 3, 2023","type":0,"sectionRef":"#","url":"/journal/2023/07/03","content":"As a backend engineer, there are several core skills that are important for success in the field. These skills include: Programming languages: Proficiency in one or more programming languages is crucial for backend development. Common languages for backend engineering include Python, Java, C#, Ruby, and JavaScript (Node.js). It's important to have a strong understanding of data structures, algorithms, and object-oriented programming concepts. Web frameworks: Familiarity with backend web frameworks is essential. Depending on the language you work with, you should be proficient in frameworks such as Django (Python), Spring (Java), ASP.NET (C#), Ruby on Rails (Ruby), or Express.js (Node.js). These frameworks provide tools and libraries for building robust web applications and services. Databases and query languages: Backend engineers often work with databases to store and retrieve data. Understanding relational databases like MySQL, PostgreSQL, or Oracle, as well as NoSQL databases like MongoDB or Redis, is valuable. Additionally, knowledge of SQL (Structured Query Language) for database querying is important. API development and integration: Backend engineers frequently design and build APIs (Application Programming Interfaces) to enable communication between different systems and services. You should have experience in designing and implementing RESTful APIs and be familiar with tools like Swagger or OpenAPI. Server management and deployment: Understanding server management and deployment processes is essential. Familiarity with cloud platforms like AWS (Amazon Web Services), Azure, or Google Cloud, as well as containerization technologies like Docker and orchestration tools like Kubernetes, is valuable. Security and scalability: Backend engineers need to have a strong understanding of security principles to develop secure applications. Knowledge of authentication and authorization mechanisms, data encryption, and handling user input securely is important. Additionally, understanding scalability concepts and techniques for handling high traffic and load balancing can be beneficial. Testing and debugging: Proficiency in testing and debugging is crucial to ensure the reliability and stability of backend systems. Knowledge of unit testing frameworks, integration testing, and debugging tools is important to identify and fix issues efficiently. Version control systems: Proficiency in version control systems like Git is essential for collaboration and managing code repositories. Understanding branching, merging, and pull requests is important to work effectively in a team. Continuous Integration and Continuous Deployment (CI/CD): Familiarity with CI/CD practices and tools like Jenkins, Travis CI, or CircleCI is valuable. Knowledge of automated testing, build pipelines, and deployment workflows is important to streamline development processes. Problem-solving and analytical thinking: Backend engineers often face complex problems that require analytical thinking and problem-solving skills. The ability to analyze requirements, break down problems into manageable tasks, and devise efficient solutions is highly valuable. Remember, the specific skills required may vary depending on the organization, industry, and technology stack being used. It's important to stay updated with emerging technologies and trends in backend development to remain competitive in the job market. Why Google Search Journey? Google Search Journey groups pages from your search histories by topic or intent, providing a more helpful user experience than just showing a chronological list of pages. Search can, at times, become a rabbit hole. A user may start out looking to book a trip but then get distracted by work or life- or doing several related searches for things they'll need for that trip - and totally forget to book the actual trip. Practice programming skills leetcodecodewars","keywords":""},{"title":"Tuesday, July 4, 2023","type":0,"sectionRef":"#","url":"/journal/2023/07/04","content":"Resumable upload app_resumable_uploadapp_resumable_upload Implementations | tus.io Resumable file upload GitHub - tus/tus-js-client: A pure JavaScript client for the tus resumable upload protocol GitHub - tus/tusd: Reference server implementation in Go of tus: the open protocol for resumable file uploads IO, StreamIO, FileIO high-level used by asyncio.io in socket/tcp/http: Streams — Python 3.11.4 documentation starlette.Request.stream = http Request Body low-level: io — Core tools for working with streams — Python 3.11.4 documentation","keywords":""},{"title":"Thursday, July 14, 2023","type":0,"sectionRef":"#","url":"/journal/2023/07/14","content":"Best practice: Update markdown metadata such as datetime when saving filesIntroduction | Front Matter Docusaurus refer code snippets from GitHub repositoriesGitHub - saucelabs/docusaurus-theme-github-codeblock: A Docusaurus v2 plugin that supports referencing code examples from public GitHub repositories. src/theme/ReferenceCodeBlock/index.tsx loading... See full example on GitHub code-snippets/XKeyIn.cpp loading... See full example on GitHub Test-Driven Development mindset involving CI, CD, documentation, iterative deliveries Create a local volume to bind a specific local folder, only available in Linux below. docker volume create --opt type=none --opt o=bind --opt device=/data/volumes/testvol testvol ➜ ~ docker inspect testvol [ { &quot;CreatedAt&quot;: &quot;2023-07-13T04:36:16Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/testvol/_data&quot;, &quot;Name&quot;: &quot;testvol&quot;, &quot;Options&quot;: { &quot;device&quot;: &quot;/data/volumes/testvol&quot;, &quot;o&quot;: &quot;bind&quot;, &quot;type&quot;: &quot;none&quot; }, &quot;Scope&quot;: &quot;local&quot; } In default, the created volume will just sit on /var/lib/docker/volumes docker volume create defaultvol ➜ ~ docker volume inspect defaultvol [ { &quot;CreatedAt&quot;: &quot;2023-07-13T04:51:57Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/defaultvol/_data&quot;, &quot;Name&quot;: &quot;defaultvol&quot;, &quot;Options&quot;: null, &quot;Scope&quot;: &quot;local&quot; } Proxies Server: Traefik vs NGINIX Proxies have become an essential networking component and are frequently used with many popular internet services. Proxy servers facilitate requests and responses between end-users and web servers, providing helpful features that augment routing control, privacy, and security. NGINX and Traefik are the most popular tools currently offering proxy functionality. Both solutions can support traditional server-based deployments and containerized application environments, such as Kubernetes. This article will examine both tools in-depth and cover their pros, cons, and distinguishing features. Traefik vs NGINX: Use Case Comparison","keywords":""},{"title":"Sunday, July 16, 2023","type":0,"sectionRef":"#","url":"/journal/2023/07/16","content":"Admission program requirements | University of Ottawa Faculty of Graduate Studies | University of Calgary Graduate Programs - University of Alberta Temporary Foreign Workers - Job Bank 20 Common Resume Buzzwords (and What to Use Instead)","keywords":""},{"title":"Thursday, July 20, 2023","type":0,"sectionRef":"#","url":"/journal/2023/07/20","content":" HapiJS Hapi.js — Project Structure and Best Practices (Part 2) Optimizing HapiJS for Benchmarks. In the past year or so, our team… | by Joel Chen | Walmart Global Tech Blog | Medium The confused saying in Microservices: &quot;each service should own its own database and no two services should share a database&quot; No golden rule, no fast rules, no best practices suitable for all businesses, only tradeoff Q: Need separate database per service? A: Creating a separate database for each service helps to enforce domain boundaries. The Hardest Part About Microservices: Your Data – Software Blog Nodejs development practices Set default configs: author name, author email, author url, the license, and the version. npm set init.author.name &quot;Your name&quot; npm set init.author.email &quot;your@email.com&quot; npm set init.author.url &quot;https://your-url.com&quot; npm set init.license &quot;MIT&quot; npm set init.version &quot;1.0.0&quot; function node-project { git init npx license $(npm get init.license) -o &quot;$(npm get init.author.name)&quot; &gt; LICENSE npx gitignore node npx covgen &quot;$(npm get init.author.email)&quot; npm init -y git add -A git commit -m &quot;Initial commit&quot; } Setting up efficient workflows with ESLint, Prettier and TypeScript in vscode. Setting up efficient workflows with ESLint, Prettier and TypeScript - JavaScript inDepth","keywords":""},{"title":"Wednesday, July 19, 2023","type":0,"sectionRef":"#","url":"/journal/2023/07/19","content":" I still prefer os.path over Pathlib, as follows Consistency, I'm used to use path string as an argument between functions and I think Pathlib is not flexible enough to handle argumentsPure and Function, Although Pathlib brings many useful features like glob, stem, and so on. I still like the concept of simplicity that don't put all things together! Trim $ for clipboard copy in Docusaurus in code block bash. Ignore $ for clipboard copy · Issue #1745 · facebook/docusaurus · GitHub Some common issues I often hit when using git Configure username/password for different repos or remotes Global configuration git config --global --list git config --local --list GIT two popular authentication methods: ssh key How to Authenticate Your Git to GitHub with SSH Keys git credentials Store username/password instead of ssh for multiple remotes To enable git credentials # local git config credential.helper store # global git config --global credential.helper store Each credential is stored in ~/.git-credentials file on its own line as a URL like: https://&lt;USERNAME&gt;:&lt;PASSWORD&gt;@github.com Configure credentials, # Global git config --global credential.https://github.com.username &lt;your_username&gt; # Or git config --local user.name &lt;your_username&gt; git config --local user.email &lt;your_useremail&gt; # Then git pull or git push to let it cache your username/password after it prompt you to input password in the first time Alternatively, we can directly edit our global Git config file ~/.gitconfig, [credential &quot;https://github.com&quot;] username = &lt;username&gt; Git - Config Username &amp; Password - Store Credentials - ShellHacks Configuring Git Credentials Programming Algorithms Top Algorithms Every Programmer Should Know What is Algorithm | Introduction to Algorithms - GeeksforGeeks","keywords":""},{"title":"Friday, July 21, 2023","type":0,"sectionRef":"#","url":"/journal/2023/07/21","content":" Mysql, redis, or other db connections pool vs as single connection in Nodejs Since Node.js and Redis are both effectively single threaded there is no need to use multiple client instances or any pooling mechanism save for a few exceptions;","keywords":""},{"title":"Tuesday, July 25, 2023","type":0,"sectionRef":"#","url":"/journal/2023/07/25","content":" Authentication and Authorization in Microservices Authentication in microservices involves two main occasions: authentication required when end users communicate with services.authentication happens between internal services.authentication needed when external services enter internal services. OAuth 2.0 provides the industry-standard protocol for authorizing users in distributed systems. The OAuth framework reduces the burden on developers, eliminating duplications to build their own authentication mechanism in each microservice. Authentication &amp; Authorization in Microservices Architecture - Part I https://softwareengineering.stackexchange.com/questions/366815/microservice-architecture-using-auth-server-as-a-user-resource-server https://auth0.com/docs/get-started User registration flow in microservice Communication between microservices Share user data between micro services User service and Comment service populate user data into Comment service, save user data in comment service, update user data in comment service https://stackoverflow.com/questions/67543408/microservices-storing-user-data-in-separate-database Ideally, the client communicates with the each service directly, and no interaction between the services! However, there is the need for communication between these services. For example, o what happens if a user deletes his account? What if you delete a TV show? You probably want to trigger some events that will update the data in your comment microservice. In the long run you want to keep everything &quot;eventually consistent&quot;. The event-driven architecture comes up! Data retrieved from two or more services For example, you send a request from UI saying &quot;give me comments with usernames&quot;, GraphQL interface then first issues a request to comments service, then to user service and finally sends one response with combined data NOTE: issue a number of requests to various micro-services to collect all the data and return it in only 1 response Rest needs to send many. https://softwareengineering.stackexchange.com/questions/418153/design-a-correct-microservices-architecture-with-data-relations Event-Driven Data Management for Microservices - NGINX","keywords":""},{"title":"Wednesday, July 26, 2023","type":0,"sectionRef":"#","url":"/journal/2023/07/26","content":"postgres + node + data model(typescript) Build a Data Access Layer with PostgreSQL and Node.js | AppSignal Blog","keywords":""},{"title":"Friday, August 4, 2023","type":0,"sectionRef":"#","url":"/journal/2023/08/04","content":" Authentication and Authorization in Microservices Authentication and Authorization in each serviceAuthentication in a centralized service, and Authorization in each serviceAuthentication and Authorization in a centralized service Auth Service and User (Profile) Service Never write a UserService again. Or when to use external Microservices Microservices Authentication Best Strategy | Aspecto Authentication and Authorization Concepts for MicroServices · GitHub design - Microservice Architecture - using Auth Server as a User Resource server How to Run Your Own Decentralized Authentication Service Using AuthN Implement event-driven architecture microservices using Redis Using Redis as an Event Store for Communication Between Microservices","keywords":""},{"title":"Monday, August 14, 2023","type":0,"sectionRef":"#","url":"/journal/2023/08/14","content":" Write best ChatGPT prompts https://stackoverflow.com/questions/6760685/creating-a-singleton-in-python https://codereview.stackexchange.com/questions/31789/progress-report-for-a-long-running-process-using-yield","keywords":""}]